{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from balm.data import load_dataset, DataCollator\n",
    "from balm.embedding import RelativePositionalEmbedding\n",
    "from balm.loss import router_z_loss\n",
    "from balm.models import BalmExpertChoiceMoEForMaskedLM\n",
    "from balm.modules import (\n",
    "    Expert,\n",
    "    BalmLMHead,\n",
    "    MaskedLMOutput,\n",
    "    TransformerLayer,\n",
    "    SparseTransformerLayer,\n",
    "    SparseMLP,\n",
    ")\n",
    "from balm.router import TopKRouter, ExpertChoiceRouter\n",
    "from balm.tokenizer import Tokenizer\n",
    "from balm.training.trainer import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TopKRouter(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"token choice of top-k experts\" strategy introduced in the\n",
    "#     `Switch Transformers`_ paper. Tokens are routed to their expert of choice until the\n",
    "#     expert's `expert_capacity` is reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         top_k: int = 1,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.top_k = top_k\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.classifier = nn.Linear(\n",
    "#             embed_dim,\n",
    "#             self.num_experts,\n",
    "#             bias=bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "#         self.jitter = jitter\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to top-k experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         top_k : int\n",
    "#             Number of top experts to route each token to.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_indices : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which experts the token should be routed to.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         top_k_values, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "#         expert_indices = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(\n",
    "#             dim=-2\n",
    "#         )\n",
    "\n",
    "#         # mask tokens if their desired experts are above capacity\n",
    "#         token_priority = torch.cumsum(expert_indices, dim=-2)\n",
    "#         expert_capacity_mask = token_priority <= self.expert_capacity\n",
    "#         expert_indices = expert_indices * expert_capacity_mask\n",
    "\n",
    "#         # get the probabilities of the top-choice experts for each token\n",
    "#         router_probs = top_k_values * expert_indices\n",
    "\n",
    "#         return expert_indices, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExpertChoiceRouter(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"expert choice of top-k tokens\" strategy introduced in the\n",
    "#     `Switch Transformers`_ paper. Tokens are routed to their expert of choice until the\n",
    "#     expert's `expert_capacity` is reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.classifier = nn.Linear(\n",
    "#             embed_dim,\n",
    "#             self.num_experts,\n",
    "#             bias=bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "#         self.jitter = jitter\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts, selecting top-k tokens for each expert.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_mask : torch.Tensor\n",
    "#             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which tokens are selected for each expert.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "#         # Select top-k tokens for each expert\n",
    "#         for i in range(self.num_experts):\n",
    "#             _, top_k_indices = torch.topk(\n",
    "#                 router_probs[..., i], k=self.expert_capacity, dim=1\n",
    "#             )\n",
    "#             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "#         # Ensure that the mask is binary\n",
    "#         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "#         return expert_mask, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RouterBase(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Base class for routers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         num_routable_experts: Optional[int] = None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.bias = bias\n",
    "#         self.jitter = jitter\n",
    "#         self.classifier = nn.Linear(\n",
    "#             self.embed_dim,\n",
    "#             num_routable_experts if num_routable_experts is not None else self.num_experts,\n",
    "#             bias=self.bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "\n",
    "# class TopKRouter(RouterBase):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"token choice of top-k experts\" strategy. For example, if k=1, this\n",
    "#     replicates the top-1 routing strategy introduced in the `Switch Transformers`_ paper.\n",
    "#     Alternatively, if k=2, this replicates the top-2 routing strategy introduced in the `GShard`_\n",
    "#     paper. Tokens are routed to their expert of choice until the expert's `expert_capacity` is\n",
    "#     reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "\n",
    "#     .. _GShard:\n",
    "#         https://arxiv.org/abs/2006.16668\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         top_k: int = 1,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             dtype=dtype,\n",
    "#             bias=bias,\n",
    "#             jitter=jitter,\n",
    "#         )\n",
    "#         self.top_k = top_k\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     # def _compute_router_probabilities(\n",
    "#     #     self, x: torch.Tensor\n",
    "#     # ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#     #     \"\"\"\n",
    "#     #     Computes router probabilities from input hidden states.\n",
    "\n",
    "#     #     Parameters:\n",
    "#     #     -----------\n",
    "#     #     x : torch.Tensor\n",
    "#     #         Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#     #         router probabilities are computed.\n",
    "\n",
    "#     #     Returns:\n",
    "#     #     --------\n",
    "#     #     router_probabilities : torch.Tensor\n",
    "#     #         Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#     #         the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#     #     router_logits : torch.Tensor\n",
    "#     #         Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#     #         to raw router logits. This is used for computing router z-loss.\n",
    "#     #     \"\"\"\n",
    "#     #     # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#     #     # https://arxiv.org/abs/2101.03961.\n",
    "#     #     # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#     #     self.input_dtype = x.dtype\n",
    "#     #     x = x.to(self.dtype)\n",
    "#     #     if self.jitter > 0:\n",
    "#     #         x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#     #     # shape: [batch_size, sequence_length, num_experts]\n",
    "#     #     logits = self.classifier(x)\n",
    "\n",
    "#     #     # apply softmax and cast back to the original dtype\n",
    "#     #     probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#     #     return probabilities, logits\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to top-k experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         top_k : int\n",
    "#             Number of top experts to route each token to.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_indices : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which experts the token should be routed to.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         top_k_values, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "#         expert_indices = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(\n",
    "#             dim=-2\n",
    "#         )\n",
    "\n",
    "#         # mask tokens if their desired experts are above capacity\n",
    "#         token_priority = torch.cumsum(expert_indices, dim=-2)\n",
    "#         expert_capacity_mask = token_priority <= self.expert_capacity\n",
    "#         expert_indices = expert_indices * expert_capacity_mask\n",
    "\n",
    "#         # get the probabilities of the top-choice experts for each token\n",
    "#         router_probs = top_k_values * expert_indices\n",
    "\n",
    "#         return expert_indices, router_probs, router_logits\n",
    "\n",
    "\n",
    "# # class ExpertChoiceRouter(RouterBase):\n",
    "# #     \"\"\"\n",
    "# #     This router uses the \"experts choice\" routing strategy introduced in the\n",
    "# #     `Mixture-of-Experts with Expert Choice Routing`_ paper. Each expert selects\n",
    "# #     its own tokens up to `expert_capacity`.\n",
    "\n",
    "# #     .. note::\n",
    "# #         There is no guarantee that each token will be processed by an expert,\n",
    "# #         or that every expert will receive at least one token. In fact, one of the\n",
    "# #         primary benefits of this router is that it allows each expert to select\n",
    "# #         its own tokens, often leading to heterogeneous token distributions among\n",
    "# #         the experts.\n",
    "\n",
    "# #     If tokens are not selected by any expert, they are passed to the subsequent\n",
    "# #     layer unchanged.\n",
    "\n",
    "\n",
    "# #     Parameters:\n",
    "# #     -----------\n",
    "# #     embed_dim : int\n",
    "# #         Embedding dimension.\n",
    "\n",
    "# #     num_experts : int\n",
    "# #         Number of experts.\n",
    "\n",
    "# #     expert_capacity : int\n",
    "# #         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "# #     dtype : str, optional\n",
    "# #         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "# #     bias : bool, optional\n",
    "# #         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "# #     jitter : float, optional\n",
    "# #         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "# #     ignore_padding_tokens : bool, optional\n",
    "# #         Whether to ignore padding tokens when computing router probabilities.\n",
    "# #         The default is ``True``.\n",
    "\n",
    "\n",
    "# #     .. _Mixture-of-Experts with Expert Choice Routing:\n",
    "# #         https://arxiv.org/abs/2202.09368\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     def __init__(\n",
    "# #         self,\n",
    "# #         embed_dim: int,\n",
    "# #         num_experts: int,\n",
    "# #         expert_capacity: int,\n",
    "# #         dtype: str = \"float32\",\n",
    "# #         bias: bool = False,\n",
    "# #         jitter: float = 0.0,\n",
    "# #         ignore_padding_tokens: bool = True,\n",
    "# #         **kwargs,\n",
    "# #     ):\n",
    "# #         super().__init__(\n",
    "# #             embed_dim=embed_dim,\n",
    "# #             num_experts=num_experts,\n",
    "# #             expert_capacity=expert_capacity,\n",
    "# #             dtype=dtype,\n",
    "# #             bias=bias,\n",
    "# #             jitter=jitter,\n",
    "# #         )\n",
    "# #         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "# #     # def _compute_router_probabilities(\n",
    "# #     #     self, x: torch.Tensor\n",
    "# #     # ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "# #     #     \"\"\"\n",
    "# #     #     Computes router probabilities from input hidden states.\n",
    "\n",
    "# #     #     Parameters:\n",
    "# #     #     -----------\n",
    "# #     #     x : torch.Tensor\n",
    "# #     #         Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "# #     #         router probabilities are computed.\n",
    "\n",
    "# #     #     Returns:\n",
    "# #     #     --------\n",
    "# #     #     router_probabilities : torch.Tensor\n",
    "# #     #         Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "# #     #         the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "# #     #     router_logits : torch.Tensor\n",
    "# #     #         Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "# #     #         to raw router logits. This is used for computing router z-loss.\n",
    "# #     #     \"\"\"\n",
    "# #     #     # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "# #     #     # https://arxiv.org/abs/2101.03961.\n",
    "# #     #     # we also store the input dtype so we can cast the output back to the original dtype\n",
    "# #     #     self.input_dtype = x.dtype\n",
    "# #     #     x = x.to(self.dtype)\n",
    "# #     #     if self.jitter > 0:\n",
    "# #     #         x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "# #     #     # shape: [batch_size, sequence_length, num_experts]\n",
    "# #     #     logits = self.classifier(x)\n",
    "\n",
    "# #     #     # apply softmax and cast back to the original dtype\n",
    "# #     #     probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "# #     #     return probabilities, logits\n",
    "\n",
    "# #     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "# #         \"\"\"\n",
    "# #         Route tokens to experts, selecting top-k tokens for each expert.\n",
    "\n",
    "# #         Parameters:\n",
    "# #         -----------\n",
    "# #         x : torch.Tensor\n",
    "# #             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "# #         Returns:\n",
    "# #         --------\n",
    "# #         expert_mask : torch.Tensor\n",
    "# #             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "# #             which tokens are selected for each expert.\n",
    "\n",
    "# #         router_probabilities : torch.Tensor\n",
    "# #             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "# #             the router probabilities.\n",
    "\n",
    "# #         router_logits : torch.Tensor\n",
    "# #             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "# #             the router logits.\n",
    "# #         \"\"\"\n",
    "# #         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "# #         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "# #         # Select top-k tokens for each expert\n",
    "# #         for i in range(self.num_experts):\n",
    "# #             _, top_k_indices = torch.topk(\n",
    "# #                 router_probs[..., i], k=self.expert_capacity, dim=1\n",
    "# #             )\n",
    "# #             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "# #         # Ensure that the mask is binary\n",
    "# #         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "# #         return expert_mask, router_probs, router_logits\n",
    "\n",
    "\n",
    "# class ExpertChoiceRouter(RouterBase):\n",
    "#     \"\"\"\n",
    "#     Router that selects top-k tokens for each expert and has shared experts that process all tokens.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 1,  # Number of shared experts\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             dtype=dtype,\n",
    "#             bias=bias,\n",
    "#             jitter=jitter,\n",
    "#             num_routable_experts=num_experts - num_shared_experts,\n",
    "#         )\n",
    "#         self.num_shared_experts = num_shared_experts\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts, selecting top-k tokens for each expert, and route all tokens to shared experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_mask : torch.Tensor\n",
    "#             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which tokens are selected for each expert and which are processed by shared experts.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "#         # Select top-k tokens for each expert\n",
    "#         for i in range(self.num_experts - self.num_shared_experts):\n",
    "#             _, top_k_indices = torch.topk(router_probs[..., i], k=self.expert_capacity, dim=1)\n",
    "#             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "#         # Ensure that the mask is binary\n",
    "#         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "#         # Add shared experts processing all tokens\n",
    "#         if self.num_shared_experts > 0:\n",
    "#             shared_expert_mask = torch.ones_like(\n",
    "#                 router_probs[..., : self.num_shared_experts]\n",
    "#             )\n",
    "#             expert_mask = torch.cat((shared_expert_mask, expert_mask), dim=-1)\n",
    "\n",
    "#         return expert_mask, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SparseMLP(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Implementation of the Switch Transformers Sparse MLP module.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     config : BalmMoEConfig\n",
    "#         Model configuration class with all the parameters of the model.\n",
    "#         Initializing with a config file does not load the weights associated with the model, only the\n",
    "#         configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\n",
    "#     router_class : nn.Module, optional\n",
    "#         Router class to use. The default is ``Router``.\n",
    "\n",
    "#     expert_class : nn.Module, optional\n",
    "#         Expert class to use. The default is ``Expert``.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         top_k: int = 1,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.router = router_class(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             top_k=top_k,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             dtype=router_dtype,\n",
    "#             bias=router_bias,\n",
    "#             jitter=router_jitter,\n",
    "#             ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#         )\n",
    "#         self.experts = nn.ModuleDict()\n",
    "#         for idx in range(num_experts):\n",
    "#             self.experts[f\"expert_{idx}\"] = expert_class(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ffn_dim=ffn_dim,\n",
    "#                 dropout_rate=expert_ffn_dropout,\n",
    "#                 activation=expert_activation,\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts and process them.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         x : torch.Tensor\n",
    "#             Output tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "#         \"\"\"\n",
    "#         # get the router mask, probabilities, and logits\n",
    "#         expert_mask, router_probs, router_logits = self.router(x)\n",
    "#         expert_outputs = []\n",
    "\n",
    "#         for idx, expert in self.experts.items():\n",
    "#             int_idx = int(idx.split(\"_\")[-1])\n",
    "#             token_indices = expert_mask[..., int_idx].bool()\n",
    "#             expert_output = expert(x[token_indices]).to(x.dtype)\n",
    "#             expanded_output = torch.zeros_like(x)\n",
    "#             expanded_output[token_indices] = expert_output\n",
    "#             expert_outputs.append(expanded_output)\n",
    "\n",
    "#         # Combine the outputs from the selected tokens for each expert\n",
    "#         x = torch.stack(expert_outputs, dim=-1) * expert_mask.unsqueeze(-2)\n",
    "#         x = x.sum(dim=-1)\n",
    "\n",
    "#         return x, (router_logits, expert_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SparseTransformerLayer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM transformer layer with Mixture of Experts. Approximately follows the ESM-2\n",
    "#     implementation, but differs in a few ways:\n",
    "#         - includes (optional) dropout for self-attention and feedforward layers\n",
    "#         - normalize **after**, not before, the self-attention and feedforward layers\n",
    "#         - we don't use rotary embeddings, which aren't (yet?) compatible with\n",
    "#           torch's optimized implementation of ``nn.MultiheadAttention``\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     config : BalmMoEConfig\n",
    "#         Model configuration class with all the parameters of the model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         top_k: int = 1,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         ffn_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.ffn_dim = ffn_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.attention_dropout = attention_dropout\n",
    "#         self.ffn_dropout = ffn_dropout\n",
    "#         self.expert_ffn_dropout = expert_ffn_dropout\n",
    "#         self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "#         # can't use rotary embeddings with nn.MultiheadAttention\n",
    "#         # see: https://discuss.pytorch.org/t/is-there-a-way-to-implement-rope-around-nn-multiheadattention-somehow/175051\n",
    "#         # it is possible to use rotary embeddings with F.scaled_dot_product_attention,\n",
    "#         # but it's not clear that it's worth the effort\n",
    "#         # see: https://github.com/pytorch/pytorch/issues/97899 for an example\n",
    "#         # self.use_rotary_embeddings = use_rotary_embeddings\n",
    "\n",
    "#         self.self_attn = nn.MultiheadAttention(\n",
    "#             embed_dim=self.embed_dim,\n",
    "#             num_heads=self.num_heads,\n",
    "#             dropout=self.attention_dropout,\n",
    "#             batch_first=attention_batch_first,\n",
    "#         )\n",
    "\n",
    "#         self.mlp = SparseMLP(\n",
    "#             embed_dim=self.embed_dim,\n",
    "#             ffn_dim=self.ffn_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             top_k=top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.ff_dropout = nn.Dropout(self.ffn_dropout)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(self.embed_dim, eps=self.layer_norm_eps)\n",
    "#         self.norm2 = nn.LayerNorm(self.embed_dim, eps=self.layer_norm_eps)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         need_weights: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#     ) -> Union[torch.Tensor, Tuple[torch.Tensor, Tuple]]:\n",
    "#         \"\"\"\n",
    "#         Process the input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         attn_mask : torch.Tensor, optional\n",
    "#             Attention mask of shape (batch_size * num_heads, sequence_length, sequence_length). The default is None.\n",
    "\n",
    "#         key_padding_mask : torch.Tensor, optional\n",
    "#             Mask of shape (batch_size, sequence_length). The default is None.\n",
    "\n",
    "#         need_weights : bool, optional\n",
    "#             Whether to return attention weights. The default is False.\n",
    "\n",
    "#             .. note::\n",
    "#                 if `need_weights` is ``True``, the output will be a tuple of (x, attn). Also,\n",
    "#                 nn.MultiHeadAttention will not be able to use the optimized torch implementation\n",
    "#                 of ``scaled_dot_product_attention``. See `here`_ for more details.\n",
    "\n",
    "#         output_router_logits : bool, optional\n",
    "#             Whether to output router logits. The default is True.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         x : torch.Tensor or Tuple\n",
    "\n",
    "#             Output tensor of shape (batch_size, sequence_length, embed_dim). If `need_weights`, is ``True``,\n",
    "#             output is a tuple of (x, attn). If `output_router_logits` is ``True``, the output will be a tuple\n",
    "#             of (x, router_logits) or (x, attn, router_logts) depending on the value of `need_weights`.\n",
    "\n",
    "\n",
    "#         .. _here:\n",
    "#             https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward\n",
    "#         \"\"\"\n",
    "#         # attention\n",
    "#         residual = x\n",
    "#         x, _ = self.self_attn(\n",
    "#             query=x,\n",
    "#             key=x,\n",
    "#             value=x,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             need_weights=need_weights,\n",
    "#             attn_mask=attention_mask,\n",
    "#         )\n",
    "#         if need_weights:\n",
    "#             x, attn = x\n",
    "#         x = residual + x\n",
    "#         x = self.norm1(x)\n",
    "\n",
    "#         # sparse feedforward\n",
    "#         residual = x\n",
    "#         x, router_tuple = self.mlp(x)  # router_tuple is (router_logits, expert_index)\n",
    "#         x = self.ff_dropout(x)\n",
    "#         x = self.norm2(residual + x)\n",
    "#         if output_router_logits and router_tuple is not None:\n",
    "#             if need_weights:\n",
    "#                 return (x, attn, router_tuple)\n",
    "#             return (x, router_tuple)\n",
    "#         if need_weights:\n",
    "#             return (x, attn)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerLayer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_heads: int,\n",
    "#         dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         activation: str = \"gelu\",\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Transformer block with relative position embeddings and GELU activation.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         embed_dim : int\n",
    "#             The input embedding dimension.\n",
    "\n",
    "#         heads : int\n",
    "#             The number of attention heads.\n",
    "\n",
    "#         forward_expansion : int\n",
    "#             The expansion factor for the feedforward network.\n",
    "\n",
    "#         max_len : int\n",
    "#             The maximum sequence length.\n",
    "\n",
    "#         dropout : float\n",
    "#             The dropout probability.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n",
    "\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=attention_dropout,\n",
    "#             batch_first=attention_batch_first,\n",
    "#         )\n",
    "\n",
    "#         activation_fn = nn.GELU() if activation.lower() == \"gelu\" else nn.ReLU()\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ffn_dim),\n",
    "#             activation_fn,\n",
    "#             nn.Linear(ffn_dim // 2, embed_dim),  # adjusted for SwiGLU\n",
    "#         )\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         need_weights: bool = False,\n",
    "#     ):\n",
    "#         # pre-norm\n",
    "#         residual = x\n",
    "#         x = self.norm1(x)\n",
    "\n",
    "#         # attention\n",
    "#         x, _ = self.attention(\n",
    "#             x,\n",
    "#             x,\n",
    "#             x,\n",
    "#             attn_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             need_weights=need_weights,\n",
    "#         )\n",
    "#         if need_weights:\n",
    "#             x, weights = x\n",
    "#         x = residual + self.dropout(x)\n",
    "\n",
    "#         # pre-norm\n",
    "#         residual = x\n",
    "#         x = self.norm2(x)\n",
    "\n",
    "#         # feedforward\n",
    "#         x = self.feed_forward(x)\n",
    "#         x = residual + self.dropout(x)\n",
    "\n",
    "#         if need_weights:\n",
    "#             return x, weights\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmMoE(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 SparseTransformerLayer(\n",
    "#                     embed_dim=embed_dim,\n",
    "#                     ffn_dim=ffn_dim,\n",
    "#                     num_heads=num_heads,\n",
    "#                     num_experts=num_experts,\n",
    "#                     num_shared_experts=num_shared_experts,\n",
    "#                     top_k=router_top_k,\n",
    "#                     expert_capacity=expert_capacity,\n",
    "#                     expert_activation=expert_activation,\n",
    "#                     expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                     attention_dropout=attention_dropout,\n",
    "#                     attention_batch_first=attention_batch_first,\n",
    "#                     layer_norm_eps=layer_norm_eps,\n",
    "#                     router_dtype=router_dtype,\n",
    "#                     router_bias=router_bias,\n",
    "#                     router_jitter=router_jitter,\n",
    "#                     router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                     router_class=router_class,\n",
    "#                     expert_class=expert_class,\n",
    "#                 )\n",
    "#                 for _ in range(num_layers)\n",
    "#             ]\n",
    "#         )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         # x = x.transpose(0, 1)\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             x = layer(\n",
    "#                 x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 key_padding_mask=key_padding_mask,\n",
    "#                 need_weights=output_attentions,\n",
    "#                 output_router_logits=output_router_logits,\n",
    "#             )\n",
    "#             if output_attentions:\n",
    "#                 x, attn, router_tuple = x\n",
    "#                 attn_weights.append(attn)\n",
    "#             else:\n",
    "#                 x, router_tuple = x\n",
    "#             router_logits.append(router_tuple[0])\n",
    "#             expert_indexes.append(router_tuple[1])\n",
    "#             if output_hidden_states:\n",
    "#                 # hidden_states[layer_idx] = x.transpose(0, 1)\n",
    "#                 hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "#         # x = x.transpose(0, 1)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         router_probs = nn.Softmax(dim=-1)(cat_router_logits)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "#         aux_loss = router_load_balancing_loss(router_probs, cat_expert_indexes)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#             router_aux_loss=aux_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmMoE(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#                 aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#                 outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss + aux_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmExpertChoiceMoE(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 SparseTransformerLayer(\n",
    "#                     embed_dim=embed_dim,\n",
    "#                     ffn_dim=ffn_dim,\n",
    "#                     num_heads=num_heads,\n",
    "#                     num_experts=num_experts,\n",
    "#                     num_shared_experts=num_shared_experts,\n",
    "#                     top_k=router_top_k,\n",
    "#                     expert_capacity=expert_capacity,\n",
    "#                     expert_activation=expert_activation,\n",
    "#                     expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                     attention_dropout=attention_dropout,\n",
    "#                     attention_batch_first=attention_batch_first,\n",
    "#                     layer_norm_eps=layer_norm_eps,\n",
    "#                     router_dtype=router_dtype,\n",
    "#                     router_bias=router_bias,\n",
    "#                     router_jitter=router_jitter,\n",
    "#                     router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                     router_class=router_class,\n",
    "#                     expert_class=expert_class,\n",
    "#                 )\n",
    "#                 for _ in range(num_layers)\n",
    "#             ]\n",
    "#         )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         # x = x.transpose(0, 1)\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             x = layer(\n",
    "#                 x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 key_padding_mask=key_padding_mask,\n",
    "#                 need_weights=output_attentions,\n",
    "#                 output_router_logits=output_router_logits,\n",
    "#             )\n",
    "#             if output_attentions:\n",
    "#                 x, attn, router_tuple = x\n",
    "#                 attn_weights.append(attn)\n",
    "#             else:\n",
    "#                 x, router_tuple = x\n",
    "#             router_logits.append(router_tuple[0])\n",
    "#             expert_indexes.append(router_tuple[1])\n",
    "#             if output_hidden_states:\n",
    "#                 # hidden_states[layer_idx] = x.transpose(0, 1)\n",
    "#                 hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "#         # x = x.transpose(0, 1)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         # router_probs = nn.Softmax(dim=-1)(cat_router_logits)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "#         # aux_loss = router_load_balancing_loss(router_probs, cat_expert_indexes)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#             # router_aux_loss=aux_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmExpertChoiceMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         # router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmExpertChoiceMoE(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         # self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         # router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#             #     aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#             #     outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmExpertChoiceMoEModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         alternate_sparsity: bool = False,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.alternate_sparsity = alternate_sparsity\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         # alternate between sparse and dense layers (dense first)\n",
    "#         if self.alternate_sparsity:\n",
    "#             layers = []\n",
    "#             for layer_num in range(num_layers):\n",
    "#                 if layer_num % 2 == 0:\n",
    "#                     layers.append(\n",
    "#                         TransformerLayer(\n",
    "#                             embed_dim=embed_dim,\n",
    "#                             ffn_dim=ffn_dim,\n",
    "#                             num_heads=num_heads,\n",
    "#                             attention_dropout=attention_dropout,\n",
    "#                             attention_batch_first=attention_batch_first,\n",
    "#                             layer_norm_eps=layer_norm_eps,\n",
    "#                             activation=expert_activation,\n",
    "#                         )\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     layers.append(\n",
    "#                         SparseTransformerLayer(\n",
    "#                             embed_dim=embed_dim,\n",
    "#                             ffn_dim=ffn_dim,\n",
    "#                             num_heads=num_heads,\n",
    "#                             num_experts=num_experts,\n",
    "#                             num_shared_experts=num_shared_experts,\n",
    "#                             top_k=router_top_k,\n",
    "#                             expert_capacity=expert_capacity,\n",
    "#                             expert_activation=expert_activation,\n",
    "#                             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                             attention_dropout=attention_dropout,\n",
    "#                             attention_batch_first=attention_batch_first,\n",
    "#                             layer_norm_eps=layer_norm_eps,\n",
    "#                             router_dtype=router_dtype,\n",
    "#                             router_bias=router_bias,\n",
    "#                             router_jitter=router_jitter,\n",
    "#                             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                             router_class=router_class,\n",
    "#                             expert_class=expert_class,\n",
    "#                         )\n",
    "#                     )\n",
    "#             self.layers = nn.ModuleList(layers)\n",
    "#         # all sparse layers\n",
    "#         else:\n",
    "#             self.layers = nn.ModuleList(\n",
    "#                 [\n",
    "#                     SparseTransformerLayer(\n",
    "#                         embed_dim=embed_dim,\n",
    "#                         ffn_dim=ffn_dim,\n",
    "#                         num_heads=num_heads,\n",
    "#                         num_experts=num_experts,\n",
    "#                         num_shared_experts=num_shared_experts,\n",
    "#                         top_k=router_top_k,\n",
    "#                         expert_capacity=expert_capacity,\n",
    "#                         expert_activation=expert_activation,\n",
    "#                         expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                         attention_dropout=attention_dropout,\n",
    "#                         attention_batch_first=attention_batch_first,\n",
    "#                         layer_norm_eps=layer_norm_eps,\n",
    "#                         router_dtype=router_dtype,\n",
    "#                         router_bias=router_bias,\n",
    "#                         router_jitter=router_jitter,\n",
    "#                         router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                         router_class=router_class,\n",
    "#                         expert_class=expert_class,\n",
    "#                     )\n",
    "#                     for _ in range(num_layers)\n",
    "#                 ]\n",
    "#             )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             if layer_idx % 2 == 0 or not self.alternate_sparsity:\n",
    "#                 # sparse layer, so we need to collect router/expert info\n",
    "#                 x = layer(\n",
    "#                     x,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     key_padding_mask=key_padding_mask,\n",
    "#                     need_weights=output_attentions,\n",
    "#                     output_router_logits=output_router_logits,\n",
    "#                 )\n",
    "#                 if output_attentions:\n",
    "#                     x, attn, router_tuple = x\n",
    "#                     attn_weights.append(attn)\n",
    "#                 else:\n",
    "#                     x, router_tuple = x\n",
    "#                 router_logits.append(router_tuple[0])\n",
    "#                 expert_indexes.append(router_tuple[1])\n",
    "#                 if output_hidden_states:\n",
    "#                     hidden_states[layer_idx] = x\n",
    "#             else:\n",
    "#                 # dense layer, no router info needed\n",
    "#                 x = layer(\n",
    "#                     x,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     need_weights=output_attentions,\n",
    "#                 )\n",
    "#                 if output_attentions:\n",
    "#                     x, attn = x\n",
    "#                     attn_weights.append(attn)\n",
    "#                 if output_hidden_states:\n",
    "#                     hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmExpertChoiceMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         alternate_sparsity: bool = False,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         # router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmExpertChoiceMoEModel(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             alternate_sparsity=alternate_sparsity,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         # self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         # router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#                 #     aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#                 #     outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80f935ca94148e18402be5902f4295a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7232bb6a13429c8037710d4168f02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9858afab8e6f4c028fe8f04cc042a400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    remove_columns=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BalmMoERoPEForMaskedLM(\n",
    "model = BalmExpertChoiceMoEForMaskedLM(\n",
    "    embed_dim=256,\n",
    "    ffn_dim=1024,\n",
    "    num_experts=4,\n",
    "    num_shared_experts=0,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    alternate_sparsity=True,\n",
    "    # router_top_k=1,\n",
    "    # router_class=ExpertChoiceRouter,\n",
    "    expert_capacity=128,\n",
    "    # expert_capacity=128,\n",
    "    router_z_loss_coef=0.01,\n",
    "    # router_aux_loss_coef=0.01,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12692257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    epochs=1,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    warmup_steps=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    # per_device_eval_batch_size=32,\n",
    "    use_cpu=True,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540f98498b6c464bbda86b588bb5ef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10   | loss: 3.0970 | lm_loss: 3.0738 | router_z_loss: 0.0232 | lr: 0.000080\n",
      "step 20   | loss: 2.8362 | lm_loss: 2.8203 | router_z_loss: 0.0158 | lr: 0.000160\n",
      "step 30   | loss: 2.6580 | lm_loss: 2.6502 | router_z_loss: 0.0078 | lr: 0.000240\n",
      "step 40   | loss: 2.5220 | lm_loss: 2.5176 | router_z_loss: 0.0043 | lr: 0.000320\n",
      "step 50   | loss: 2.3141 | lm_loss: 2.3115 | router_z_loss: 0.0026 | lr: 0.000400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48459006e3bf497fa6ecc3f8c6d1f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 2.3333\n",
      "step 60   | loss: 2.2457 | lm_loss: 2.2436 | router_z_loss: 0.0020 | lr: 0.000398\n",
      "step 70   | loss: 2.0994 | lm_loss: 2.0977 | router_z_loss: 0.0017 | lr: 0.000396\n",
      "step 80   | loss: 2.1750 | lm_loss: 2.1737 | router_z_loss: 0.0013 | lr: 0.000394\n",
      "step 90   | loss: 2.0424 | lm_loss: 2.0412 | router_z_loss: 0.0012 | lr: 0.000392\n",
      "step 100  | loss: 2.0945 | lm_loss: 2.0934 | router_z_loss: 0.0011 | lr: 0.000390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fde855de54ec1b46343eaa6f02d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 2.0243\n",
      "step 110  | loss: 1.9373 | lm_loss: 1.9364 | router_z_loss: 0.0009 | lr: 0.000388\n",
      "step 120  | loss: 1.9678 | lm_loss: 1.9671 | router_z_loss: 0.0007 | lr: 0.000386\n",
      "step 130  | loss: 2.0049 | lm_loss: 2.0041 | router_z_loss: 0.0008 | lr: 0.000384\n",
      "step 140  | loss: 1.9785 | lm_loss: 1.9778 | router_z_loss: 0.0007 | lr: 0.000382\n",
      "step 150  | loss: 1.9420 | lm_loss: 1.9412 | router_z_loss: 0.0007 | lr: 0.000380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03793acbe8374286b1569c8bdda0e878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.9387\n",
      "step 160  | loss: 2.0567 | lm_loss: 2.0559 | router_z_loss: 0.0008 | lr: 0.000378\n",
      "step 170  | loss: 1.8994 | lm_loss: 1.8989 | router_z_loss: 0.0005 | lr: 0.000376\n",
      "step 180  | loss: 1.9081 | lm_loss: 1.9075 | router_z_loss: 0.0006 | lr: 0.000374\n",
      "step 190  | loss: 1.9570 | lm_loss: 1.9565 | router_z_loss: 0.0005 | lr: 0.000373\n",
      "step 200  | loss: 1.8127 | lm_loss: 1.8122 | router_z_loss: 0.0005 | lr: 0.000371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e17b2c28c3c4c82a0747efbbcd4bf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.8556\n",
      "step 210  | loss: 1.8709 | lm_loss: 1.8699 | router_z_loss: 0.0010 | lr: 0.000369\n",
      "step 220  | loss: 1.9173 | lm_loss: 1.9168 | router_z_loss: 0.0005 | lr: 0.000367\n",
      "step 230  | loss: 1.8043 | lm_loss: 1.8036 | router_z_loss: 0.0007 | lr: 0.000365\n",
      "step 240  | loss: 1.8686 | lm_loss: 1.8681 | router_z_loss: 0.0005 | lr: 0.000363\n",
      "step 250  | loss: 1.7909 | lm_loss: 1.7901 | router_z_loss: 0.0007 | lr: 0.000361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f88847902c49d29b094b49afdf2c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.7765\n",
      "step 260  | loss: 1.7120 | lm_loss: 1.7115 | router_z_loss: 0.0006 | lr: 0.000359\n",
      "step 270  | loss: 1.7358 | lm_loss: 1.7351 | router_z_loss: 0.0007 | lr: 0.000357\n",
      "step 280  | loss: 1.6308 | lm_loss: 1.6302 | router_z_loss: 0.0006 | lr: 0.000355\n",
      "step 290  | loss: 1.7197 | lm_loss: 1.7191 | router_z_loss: 0.0007 | lr: 0.000353\n",
      "step 300  | loss: 1.6612 | lm_loss: 1.6606 | router_z_loss: 0.0006 | lr: 0.000351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08144e3ba44e4278bd0b24cddd766ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.6162\n",
      "step 310  | loss: 1.5319 | lm_loss: 1.5313 | router_z_loss: 0.0006 | lr: 0.000349\n",
      "step 320  | loss: 1.5791 | lm_loss: 1.5785 | router_z_loss: 0.0006 | lr: 0.000347\n",
      "step 330  | loss: 1.5224 | lm_loss: 1.5219 | router_z_loss: 0.0005 | lr: 0.000345\n",
      "step 340  | loss: 1.6225 | lm_loss: 1.6220 | router_z_loss: 0.0005 | lr: 0.000343\n",
      "step 350  | loss: 1.5291 | lm_loss: 1.5286 | router_z_loss: 0.0005 | lr: 0.000341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5b00ca171a4b178abe20ae1e6e7bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.4656\n",
      "step 360  | loss: 1.4411 | lm_loss: 1.4407 | router_z_loss: 0.0005 | lr: 0.000339\n",
      "step 370  | loss: 1.4500 | lm_loss: 1.4496 | router_z_loss: 0.0005 | lr: 0.000337\n",
      "step 380  | loss: 1.3997 | lm_loss: 1.3987 | router_z_loss: 0.0010 | lr: 0.000335\n",
      "step 390  | loss: 1.4401 | lm_loss: 1.4396 | router_z_loss: 0.0005 | lr: 0.000333\n",
      "step 400  | loss: 1.3658 | lm_loss: 1.3652 | router_z_loss: 0.0006 | lr: 0.000331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc3647d8e584a65b11a4bf9d6277c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.3207\n",
      "step 410  | loss: 1.2689 | lm_loss: 1.2684 | router_z_loss: 0.0005 | lr: 0.000329\n",
      "step 420  | loss: 1.2503 | lm_loss: 1.2498 | router_z_loss: 0.0005 | lr: 0.000327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/git/BALM/balm/training/trainer.py:201\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    195\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    196\u001b[0m     labels\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    197\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    198\u001b[0m     key_padding_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 201\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
