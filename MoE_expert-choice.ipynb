{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from balm.data import load_dataset, DataCollator\n",
    "from balm.embedding import RelativePositionalEmbedding\n",
    "from balm.loss import router_z_loss\n",
    "from balm.models import BalmExpertChoiceMoEForMaskedLM\n",
    "from balm.modules import (\n",
    "    Expert,\n",
    "    BalmLMHead,\n",
    "    MaskedLMOutput,\n",
    "    TransformerLayer,\n",
    "    SparseTransformerLayer,\n",
    "    SparseMLP,\n",
    ")\n",
    "from balm.router import TopKRouter, ExpertChoiceRouter\n",
    "from balm.tokenizer import Tokenizer\n",
    "from balm.training.trainer import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TopKRouter(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"token choice of top-k experts\" strategy introduced in the\n",
    "#     `Switch Transformers`_ paper. Tokens are routed to their expert of choice until the\n",
    "#     expert's `expert_capacity` is reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         top_k: int = 1,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.top_k = top_k\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.classifier = nn.Linear(\n",
    "#             embed_dim,\n",
    "#             self.num_experts,\n",
    "#             bias=bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "#         self.jitter = jitter\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to top-k experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         top_k : int\n",
    "#             Number of top experts to route each token to.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_indices : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which experts the token should be routed to.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         top_k_values, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "#         expert_indices = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(\n",
    "#             dim=-2\n",
    "#         )\n",
    "\n",
    "#         # mask tokens if their desired experts are above capacity\n",
    "#         token_priority = torch.cumsum(expert_indices, dim=-2)\n",
    "#         expert_capacity_mask = token_priority <= self.expert_capacity\n",
    "#         expert_indices = expert_indices * expert_capacity_mask\n",
    "\n",
    "#         # get the probabilities of the top-choice experts for each token\n",
    "#         router_probs = top_k_values * expert_indices\n",
    "\n",
    "#         return expert_indices, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExpertChoiceRouter(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"expert choice of top-k tokens\" strategy introduced in the\n",
    "#     `Switch Transformers`_ paper. Tokens are routed to their expert of choice until the\n",
    "#     expert's `expert_capacity` is reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.classifier = nn.Linear(\n",
    "#             embed_dim,\n",
    "#             self.num_experts,\n",
    "#             bias=bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "#         self.jitter = jitter\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts, selecting top-k tokens for each expert.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_mask : torch.Tensor\n",
    "#             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which tokens are selected for each expert.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "#         # Select top-k tokens for each expert\n",
    "#         for i in range(self.num_experts):\n",
    "#             _, top_k_indices = torch.topk(\n",
    "#                 router_probs[..., i], k=self.expert_capacity, dim=1\n",
    "#             )\n",
    "#             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "#         # Ensure that the mask is binary\n",
    "#         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "#         return expert_mask, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RouterBase(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Base class for routers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         num_routable_experts: Optional[int] = None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_experts = num_experts\n",
    "#         self.expert_capacity = expert_capacity\n",
    "#         self.dtype = getattr(torch, dtype)\n",
    "#         self.bias = bias\n",
    "#         self.jitter = jitter\n",
    "#         self.classifier = nn.Linear(\n",
    "#             self.embed_dim,\n",
    "#             num_routable_experts if num_routable_experts is not None else self.num_experts,\n",
    "#             bias=self.bias,\n",
    "#             dtype=self.dtype,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         raise NotImplementedError\n",
    "\n",
    "#     def _compute_router_probabilities(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes router probabilities from input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#             router probabilities are computed.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#             the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#             to raw router logits. This is used for computing router z-loss.\n",
    "#         \"\"\"\n",
    "#         # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#         # https://arxiv.org/abs/2101.03961.\n",
    "#         # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#         self.input_dtype = x.dtype\n",
    "#         x = x.to(self.dtype)\n",
    "#         if self.jitter > 0:\n",
    "#             x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#         # shape: [batch_size, sequence_length, num_experts]\n",
    "#         logits = self.classifier(x)\n",
    "\n",
    "#         # apply softmax and cast back to the original dtype\n",
    "#         probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#         return probabilities, logits\n",
    "\n",
    "\n",
    "# class TopKRouter(RouterBase):\n",
    "#     \"\"\"\n",
    "#     This router uses the \"token choice of top-k experts\" strategy. For example, if k=1, this\n",
    "#     replicates the top-1 routing strategy introduced in the `Switch Transformers`_ paper.\n",
    "#     Alternatively, if k=2, this replicates the top-2 routing strategy introduced in the `GShard`_\n",
    "#     paper. Tokens are routed to their expert of choice until the expert's `expert_capacity` is\n",
    "#     reached.\n",
    "\n",
    "#     .. note::\n",
    "#         There is no guarantee that each token will be processed by an expert,\n",
    "#         or that every expert will receive at least one token.\n",
    "\n",
    "#     If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "#     and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     embed_dim : int\n",
    "#         Embedding dimension.\n",
    "\n",
    "#     num_experts : int\n",
    "#         Number of experts.\n",
    "\n",
    "#     expert_capacity : int\n",
    "#         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "#     dtype : str, optional\n",
    "#         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "#     bias : bool, optional\n",
    "#         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "#     jitter : float, optional\n",
    "#         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "#     ignore_padding_tokens : bool, optional\n",
    "#         Whether to ignore padding tokens when computing router probabilities.\n",
    "#         The default is ``True``.\n",
    "\n",
    "\n",
    "#     .. _Switch Transformers:\n",
    "#         https://arxiv.org/abs/2101.03961\n",
    "\n",
    "#     .. _GShard:\n",
    "#         https://arxiv.org/abs/2006.16668\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         top_k: int = 1,\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             dtype=dtype,\n",
    "#             bias=bias,\n",
    "#             jitter=jitter,\n",
    "#         )\n",
    "#         self.top_k = top_k\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     # def _compute_router_probabilities(\n",
    "#     #     self, x: torch.Tensor\n",
    "#     # ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#     #     \"\"\"\n",
    "#     #     Computes router probabilities from input hidden states.\n",
    "\n",
    "#     #     Parameters:\n",
    "#     #     -----------\n",
    "#     #     x : torch.Tensor\n",
    "#     #         Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "#     #         router probabilities are computed.\n",
    "\n",
    "#     #     Returns:\n",
    "#     #     --------\n",
    "#     #     router_probabilities : torch.Tensor\n",
    "#     #         Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "#     #         the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "#     #     router_logits : torch.Tensor\n",
    "#     #         Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "#     #         to raw router logits. This is used for computing router z-loss.\n",
    "#     #     \"\"\"\n",
    "#     #     # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "#     #     # https://arxiv.org/abs/2101.03961.\n",
    "#     #     # we also store the input dtype so we can cast the output back to the original dtype\n",
    "#     #     self.input_dtype = x.dtype\n",
    "#     #     x = x.to(self.dtype)\n",
    "#     #     if self.jitter > 0:\n",
    "#     #         x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "#     #     # shape: [batch_size, sequence_length, num_experts]\n",
    "#     #     logits = self.classifier(x)\n",
    "\n",
    "#     #     # apply softmax and cast back to the original dtype\n",
    "#     #     probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "#     #     return probabilities, logits\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to top-k experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         top_k : int\n",
    "#             Number of top experts to route each token to.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_indices : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which experts the token should be routed to.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         top_k_values, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "#         expert_indices = F.one_hot(top_k_indices, num_classes=self.num_experts).sum(\n",
    "#             dim=-2\n",
    "#         )\n",
    "\n",
    "#         # mask tokens if their desired experts are above capacity\n",
    "#         token_priority = torch.cumsum(expert_indices, dim=-2)\n",
    "#         expert_capacity_mask = token_priority <= self.expert_capacity\n",
    "#         expert_indices = expert_indices * expert_capacity_mask\n",
    "\n",
    "#         # get the probabilities of the top-choice experts for each token\n",
    "#         router_probs = top_k_values * expert_indices\n",
    "\n",
    "#         return expert_indices, router_probs, router_logits\n",
    "\n",
    "\n",
    "# # class ExpertChoiceRouter(RouterBase):\n",
    "# #     \"\"\"\n",
    "# #     This router uses the \"experts choice\" routing strategy introduced in the\n",
    "# #     `Mixture-of-Experts with Expert Choice Routing`_ paper. Each expert selects\n",
    "# #     its own tokens up to `expert_capacity`.\n",
    "\n",
    "# #     .. note::\n",
    "# #         There is no guarantee that each token will be processed by an expert,\n",
    "# #         or that every expert will receive at least one token. In fact, one of the\n",
    "# #         primary benefits of this router is that it allows each expert to select\n",
    "# #         its own tokens, often leading to heterogeneous token distributions among\n",
    "# #         the experts.\n",
    "\n",
    "# #     If tokens are not selected by any expert, they are passed to the subsequent\n",
    "# #     layer unchanged.\n",
    "\n",
    "\n",
    "# #     Parameters:\n",
    "# #     -----------\n",
    "# #     embed_dim : int\n",
    "# #         Embedding dimension.\n",
    "\n",
    "# #     num_experts : int\n",
    "# #         Number of experts.\n",
    "\n",
    "# #     expert_capacity : int\n",
    "# #         Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "# #     dtype : str, optional\n",
    "# #         Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "# #     bias : bool, optional\n",
    "# #         Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "# #     jitter : float, optional\n",
    "# #         Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "# #     ignore_padding_tokens : bool, optional\n",
    "# #         Whether to ignore padding tokens when computing router probabilities.\n",
    "# #         The default is ``True``.\n",
    "\n",
    "\n",
    "# #     .. _Mixture-of-Experts with Expert Choice Routing:\n",
    "# #         https://arxiv.org/abs/2202.09368\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     def __init__(\n",
    "# #         self,\n",
    "# #         embed_dim: int,\n",
    "# #         num_experts: int,\n",
    "# #         expert_capacity: int,\n",
    "# #         dtype: str = \"float32\",\n",
    "# #         bias: bool = False,\n",
    "# #         jitter: float = 0.0,\n",
    "# #         ignore_padding_tokens: bool = True,\n",
    "# #         **kwargs,\n",
    "# #     ):\n",
    "# #         super().__init__(\n",
    "# #             embed_dim=embed_dim,\n",
    "# #             num_experts=num_experts,\n",
    "# #             expert_capacity=expert_capacity,\n",
    "# #             dtype=dtype,\n",
    "# #             bias=bias,\n",
    "# #             jitter=jitter,\n",
    "# #         )\n",
    "# #         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "# #     # def _compute_router_probabilities(\n",
    "# #     #     self, x: torch.Tensor\n",
    "# #     # ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "# #     #     \"\"\"\n",
    "# #     #     Computes router probabilities from input hidden states.\n",
    "\n",
    "# #     #     Parameters:\n",
    "# #     #     -----------\n",
    "# #     #     x : torch.Tensor\n",
    "# #     #         Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "# #     #         router probabilities are computed.\n",
    "\n",
    "# #     #     Returns:\n",
    "# #     #     --------\n",
    "# #     #     router_probabilities : torch.Tensor\n",
    "# #     #         Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "# #     #         the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "# #     #     router_logits : torch.Tensor\n",
    "# #     #         Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "# #     #         to raw router logits. This is used for computing router z-loss.\n",
    "# #     #     \"\"\"\n",
    "# #     #     # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "# #     #     # https://arxiv.org/abs/2101.03961.\n",
    "# #     #     # we also store the input dtype so we can cast the output back to the original dtype\n",
    "# #     #     self.input_dtype = x.dtype\n",
    "# #     #     x = x.to(self.dtype)\n",
    "# #     #     if self.jitter > 0:\n",
    "# #     #         x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "# #     #     # shape: [batch_size, sequence_length, num_experts]\n",
    "# #     #     logits = self.classifier(x)\n",
    "\n",
    "# #     #     # apply softmax and cast back to the original dtype\n",
    "# #     #     probabilities = F.softmax(logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
    "# #     #     return probabilities, logits\n",
    "\n",
    "# #     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "# #         \"\"\"\n",
    "# #         Route tokens to experts, selecting top-k tokens for each expert.\n",
    "\n",
    "# #         Parameters:\n",
    "# #         -----------\n",
    "# #         x : torch.Tensor\n",
    "# #             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "# #         Returns:\n",
    "# #         --------\n",
    "# #         expert_mask : torch.Tensor\n",
    "# #             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "# #             which tokens are selected for each expert.\n",
    "\n",
    "# #         router_probabilities : torch.Tensor\n",
    "# #             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "# #             the router probabilities.\n",
    "\n",
    "# #         router_logits : torch.Tensor\n",
    "# #             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "# #             the router logits.\n",
    "# #         \"\"\"\n",
    "# #         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "# #         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "# #         # Select top-k tokens for each expert\n",
    "# #         for i in range(self.num_experts):\n",
    "# #             _, top_k_indices = torch.topk(\n",
    "# #                 router_probs[..., i], k=self.expert_capacity, dim=1\n",
    "# #             )\n",
    "# #             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "# #         # Ensure that the mask is binary\n",
    "# #         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "# #         return expert_mask, router_probs, router_logits\n",
    "\n",
    "\n",
    "# class ExpertChoiceRouter(RouterBase):\n",
    "#     \"\"\"\n",
    "#     Router that selects top-k tokens for each expert and has shared experts that process all tokens.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 1,  # Number of shared experts\n",
    "#         dtype: str = \"float32\",\n",
    "#         bias: bool = False,\n",
    "#         jitter: float = 0.0,\n",
    "#         ignore_padding_tokens: bool = True,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             dtype=dtype,\n",
    "#             bias=bias,\n",
    "#             jitter=jitter,\n",
    "#             num_routable_experts=num_experts - num_shared_experts,\n",
    "#         )\n",
    "#         self.num_shared_experts = num_shared_experts\n",
    "#         self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "#     def forward(\n",
    "#         self, x: torch.Tensor\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts, selecting top-k tokens for each expert, and route all tokens to shared experts.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         expert_mask : torch.Tensor\n",
    "#             Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "#             which tokens are selected for each expert and which are processed by shared experts.\n",
    "\n",
    "#         router_probabilities : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router probabilities.\n",
    "\n",
    "#         router_logits : torch.Tensor\n",
    "#             Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "#             the router logits.\n",
    "#         \"\"\"\n",
    "#         router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "#         expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "#         # Select top-k tokens for each expert\n",
    "#         for i in range(self.num_experts - self.num_shared_experts):\n",
    "#             _, top_k_indices = torch.topk(router_probs[..., i], k=self.expert_capacity, dim=1)\n",
    "#             expert_mask.scatter_(1, top_k_indices.unsqueeze(-1), 1, reduce=\"add\")\n",
    "\n",
    "#         # Ensure that the mask is binary\n",
    "#         expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "#         # Add shared experts processing all tokens\n",
    "#         if self.num_shared_experts > 0:\n",
    "#             shared_expert_mask = torch.ones_like(\n",
    "#                 router_probs[..., : self.num_shared_experts]\n",
    "#             )\n",
    "#             expert_mask = torch.cat((shared_expert_mask, expert_mask), dim=-1)\n",
    "\n",
    "#         return expert_mask, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SparseMLP(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Implementation of the Switch Transformers Sparse MLP module.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     config : BalmMoEConfig\n",
    "#         Model configuration class with all the parameters of the model.\n",
    "#         Initializing with a config file does not load the weights associated with the model, only the\n",
    "#         configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\n",
    "#     router_class : nn.Module, optional\n",
    "#         Router class to use. The default is ``Router``.\n",
    "\n",
    "#     expert_class : nn.Module, optional\n",
    "#         Expert class to use. The default is ``Expert``.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         top_k: int = 1,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.router = router_class(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             top_k=top_k,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             dtype=router_dtype,\n",
    "#             bias=router_bias,\n",
    "#             jitter=router_jitter,\n",
    "#             ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#         )\n",
    "#         self.experts = nn.ModuleDict()\n",
    "#         for idx in range(num_experts):\n",
    "#             self.experts[f\"expert_{idx}\"] = expert_class(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ffn_dim=ffn_dim,\n",
    "#                 dropout_rate=expert_ffn_dropout,\n",
    "#                 activation=expert_activation,\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "#         \"\"\"\n",
    "#         Route tokens to experts and process them.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         x : torch.Tensor\n",
    "#             Output tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "#         \"\"\"\n",
    "#         # get the router mask, probabilities, and logits\n",
    "#         expert_mask, router_probs, router_logits = self.router(x)\n",
    "#         expert_outputs = []\n",
    "\n",
    "#         for idx, expert in self.experts.items():\n",
    "#             int_idx = int(idx.split(\"_\")[-1])\n",
    "#             token_indices = expert_mask[..., int_idx].bool()\n",
    "#             expert_output = expert(x[token_indices]).to(x.dtype)\n",
    "#             expanded_output = torch.zeros_like(x)\n",
    "#             expanded_output[token_indices] = expert_output\n",
    "#             expert_outputs.append(expanded_output)\n",
    "\n",
    "#         # Combine the outputs from the selected tokens for each expert\n",
    "#         x = torch.stack(expert_outputs, dim=-1) * expert_mask.unsqueeze(-2)\n",
    "#         x = x.sum(dim=-1)\n",
    "\n",
    "#         return x, (router_logits, expert_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SparseTransformerLayer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM transformer layer with Mixture of Experts. Approximately follows the ESM-2\n",
    "#     implementation, but differs in a few ways:\n",
    "#         - includes (optional) dropout for self-attention and feedforward layers\n",
    "#         - normalize **after**, not before, the self-attention and feedforward layers\n",
    "#         - we don't use rotary embeddings, which aren't (yet?) compatible with\n",
    "#           torch's optimized implementation of ``nn.MultiheadAttention``\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     config : BalmMoEConfig\n",
    "#         Model configuration class with all the parameters of the model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         top_k: int = 1,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         ffn_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.ffn_dim = ffn_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.attention_dropout = attention_dropout\n",
    "#         self.ffn_dropout = ffn_dropout\n",
    "#         self.expert_ffn_dropout = expert_ffn_dropout\n",
    "#         self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "#         # can't use rotary embeddings with nn.MultiheadAttention\n",
    "#         # see: https://discuss.pytorch.org/t/is-there-a-way-to-implement-rope-around-nn-multiheadattention-somehow/175051\n",
    "#         # it is possible to use rotary embeddings with F.scaled_dot_product_attention,\n",
    "#         # but it's not clear that it's worth the effort\n",
    "#         # see: https://github.com/pytorch/pytorch/issues/97899 for an example\n",
    "#         # self.use_rotary_embeddings = use_rotary_embeddings\n",
    "\n",
    "#         self.self_attn = nn.MultiheadAttention(\n",
    "#             embed_dim=self.embed_dim,\n",
    "#             num_heads=self.num_heads,\n",
    "#             dropout=self.attention_dropout,\n",
    "#             batch_first=attention_batch_first,\n",
    "#         )\n",
    "\n",
    "#         self.mlp = SparseMLP(\n",
    "#             embed_dim=self.embed_dim,\n",
    "#             ffn_dim=self.ffn_dim,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             top_k=top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.ff_dropout = nn.Dropout(self.ffn_dropout)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(self.embed_dim, eps=self.layer_norm_eps)\n",
    "#         self.norm2 = nn.LayerNorm(self.embed_dim, eps=self.layer_norm_eps)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         need_weights: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#     ) -> Union[torch.Tensor, Tuple[torch.Tensor, Tuple]]:\n",
    "#         \"\"\"\n",
    "#         Process the input hidden states.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#         x : torch.Tensor\n",
    "#             Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "#         attn_mask : torch.Tensor, optional\n",
    "#             Attention mask of shape (batch_size * num_heads, sequence_length, sequence_length). The default is None.\n",
    "\n",
    "#         key_padding_mask : torch.Tensor, optional\n",
    "#             Mask of shape (batch_size, sequence_length). The default is None.\n",
    "\n",
    "#         need_weights : bool, optional\n",
    "#             Whether to return attention weights. The default is False.\n",
    "\n",
    "#             .. note::\n",
    "#                 if `need_weights` is ``True``, the output will be a tuple of (x, attn). Also,\n",
    "#                 nn.MultiHeadAttention will not be able to use the optimized torch implementation\n",
    "#                 of ``scaled_dot_product_attention``. See `here`_ for more details.\n",
    "\n",
    "#         output_router_logits : bool, optional\n",
    "#             Whether to output router logits. The default is True.\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         x : torch.Tensor or Tuple\n",
    "\n",
    "#             Output tensor of shape (batch_size, sequence_length, embed_dim). If `need_weights`, is ``True``,\n",
    "#             output is a tuple of (x, attn). If `output_router_logits` is ``True``, the output will be a tuple\n",
    "#             of (x, router_logits) or (x, attn, router_logts) depending on the value of `need_weights`.\n",
    "\n",
    "\n",
    "#         .. _here:\n",
    "#             https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward\n",
    "#         \"\"\"\n",
    "#         # attention\n",
    "#         residual = x\n",
    "#         x, _ = self.self_attn(\n",
    "#             query=x,\n",
    "#             key=x,\n",
    "#             value=x,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             need_weights=need_weights,\n",
    "#             attn_mask=attention_mask,\n",
    "#         )\n",
    "#         if need_weights:\n",
    "#             x, attn = x\n",
    "#         x = residual + x\n",
    "#         x = self.norm1(x)\n",
    "\n",
    "#         # sparse feedforward\n",
    "#         residual = x\n",
    "#         x, router_tuple = self.mlp(x)  # router_tuple is (router_logits, expert_index)\n",
    "#         x = self.ff_dropout(x)\n",
    "#         x = self.norm2(residual + x)\n",
    "#         if output_router_logits and router_tuple is not None:\n",
    "#             if need_weights:\n",
    "#                 return (x, attn, router_tuple)\n",
    "#             return (x, router_tuple)\n",
    "#         if need_weights:\n",
    "#             return (x, attn)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerLayer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_heads: int,\n",
    "#         dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         activation: str = \"gelu\",\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Transformer block with relative position embeddings and GELU activation.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         embed_dim : int\n",
    "#             The input embedding dimension.\n",
    "\n",
    "#         heads : int\n",
    "#             The number of attention heads.\n",
    "\n",
    "#         forward_expansion : int\n",
    "#             The expansion factor for the feedforward network.\n",
    "\n",
    "#         max_len : int\n",
    "#             The maximum sequence length.\n",
    "\n",
    "#         dropout : float\n",
    "#             The dropout probability.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim, eps=layer_norm_eps)\n",
    "\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=embed_dim,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=attention_dropout,\n",
    "#             batch_first=attention_batch_first,\n",
    "#         )\n",
    "\n",
    "#         activation_fn = nn.GELU() if activation.lower() == \"gelu\" else nn.ReLU()\n",
    "#         self.feed_forward = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ffn_dim),\n",
    "#             activation_fn,\n",
    "#             nn.Linear(ffn_dim // 2, embed_dim),  # adjusted for SwiGLU\n",
    "#         )\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         need_weights: bool = False,\n",
    "#     ):\n",
    "#         # pre-norm\n",
    "#         residual = x\n",
    "#         x = self.norm1(x)\n",
    "\n",
    "#         # attention\n",
    "#         x, _ = self.attention(\n",
    "#             x,\n",
    "#             x,\n",
    "#             x,\n",
    "#             attn_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             need_weights=need_weights,\n",
    "#         )\n",
    "#         if need_weights:\n",
    "#             x, weights = x\n",
    "#         x = residual + self.dropout(x)\n",
    "\n",
    "#         # pre-norm\n",
    "#         residual = x\n",
    "#         x = self.norm2(x)\n",
    "\n",
    "#         # feedforward\n",
    "#         x = self.feed_forward(x)\n",
    "#         x = residual + self.dropout(x)\n",
    "\n",
    "#         if need_weights:\n",
    "#             return x, weights\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmMoE(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 SparseTransformerLayer(\n",
    "#                     embed_dim=embed_dim,\n",
    "#                     ffn_dim=ffn_dim,\n",
    "#                     num_heads=num_heads,\n",
    "#                     num_experts=num_experts,\n",
    "#                     num_shared_experts=num_shared_experts,\n",
    "#                     top_k=router_top_k,\n",
    "#                     expert_capacity=expert_capacity,\n",
    "#                     expert_activation=expert_activation,\n",
    "#                     expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                     attention_dropout=attention_dropout,\n",
    "#                     attention_batch_first=attention_batch_first,\n",
    "#                     layer_norm_eps=layer_norm_eps,\n",
    "#                     router_dtype=router_dtype,\n",
    "#                     router_bias=router_bias,\n",
    "#                     router_jitter=router_jitter,\n",
    "#                     router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                     router_class=router_class,\n",
    "#                     expert_class=expert_class,\n",
    "#                 )\n",
    "#                 for _ in range(num_layers)\n",
    "#             ]\n",
    "#         )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         # x = x.transpose(0, 1)\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             x = layer(\n",
    "#                 x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 key_padding_mask=key_padding_mask,\n",
    "#                 need_weights=output_attentions,\n",
    "#                 output_router_logits=output_router_logits,\n",
    "#             )\n",
    "#             if output_attentions:\n",
    "#                 x, attn, router_tuple = x\n",
    "#                 attn_weights.append(attn)\n",
    "#             else:\n",
    "#                 x, router_tuple = x\n",
    "#             router_logits.append(router_tuple[0])\n",
    "#             expert_indexes.append(router_tuple[1])\n",
    "#             if output_hidden_states:\n",
    "#                 # hidden_states[layer_idx] = x.transpose(0, 1)\n",
    "#                 hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "#         # x = x.transpose(0, 1)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         router_probs = nn.Softmax(dim=-1)(cat_router_logits)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "#         aux_loss = router_load_balancing_loss(router_probs, cat_expert_indexes)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#             router_aux_loss=aux_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = TopKRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmMoE(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#                 aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#                 outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss + aux_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmExpertChoiceMoE(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [\n",
    "#                 SparseTransformerLayer(\n",
    "#                     embed_dim=embed_dim,\n",
    "#                     ffn_dim=ffn_dim,\n",
    "#                     num_heads=num_heads,\n",
    "#                     num_experts=num_experts,\n",
    "#                     num_shared_experts=num_shared_experts,\n",
    "#                     top_k=router_top_k,\n",
    "#                     expert_capacity=expert_capacity,\n",
    "#                     expert_activation=expert_activation,\n",
    "#                     expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                     attention_dropout=attention_dropout,\n",
    "#                     attention_batch_first=attention_batch_first,\n",
    "#                     layer_norm_eps=layer_norm_eps,\n",
    "#                     router_dtype=router_dtype,\n",
    "#                     router_bias=router_bias,\n",
    "#                     router_jitter=router_jitter,\n",
    "#                     router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                     router_class=router_class,\n",
    "#                     expert_class=expert_class,\n",
    "#                 )\n",
    "#                 for _ in range(num_layers)\n",
    "#             ]\n",
    "#         )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         # x = x.transpose(0, 1)\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             x = layer(\n",
    "#                 x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 key_padding_mask=key_padding_mask,\n",
    "#                 need_weights=output_attentions,\n",
    "#                 output_router_logits=output_router_logits,\n",
    "#             )\n",
    "#             if output_attentions:\n",
    "#                 x, attn, router_tuple = x\n",
    "#                 attn_weights.append(attn)\n",
    "#             else:\n",
    "#                 x, router_tuple = x\n",
    "#             router_logits.append(router_tuple[0])\n",
    "#             expert_indexes.append(router_tuple[1])\n",
    "#             if output_hidden_states:\n",
    "#                 # hidden_states[layer_idx] = x.transpose(0, 1)\n",
    "#                 hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "#         # x = x.transpose(0, 1)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         # router_probs = nn.Softmax(dim=-1)(cat_router_logits)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "#         # aux_loss = router_load_balancing_loss(router_probs, cat_expert_indexes)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#             # router_aux_loss=aux_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmExpertChoiceMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         # router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmExpertChoiceMoE(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         # self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         # router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#             #     aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#             #     outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BalmExpertChoiceMoEModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         alternate_sparsity: bool = False,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#         # config: BalmMoEConfig,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.alternate_sparsity = alternate_sparsity\n",
    "#         self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "#         self.embed_positions = RelativePositionalEmbedding(embed_dim)\n",
    "#         # alternate between sparse and dense layers (dense first)\n",
    "#         if self.alternate_sparsity:\n",
    "#             layers = []\n",
    "#             for layer_num in range(num_layers):\n",
    "#                 if layer_num % 2 == 0:\n",
    "#                     layers.append(\n",
    "#                         TransformerLayer(\n",
    "#                             embed_dim=embed_dim,\n",
    "#                             ffn_dim=ffn_dim,\n",
    "#                             num_heads=num_heads,\n",
    "#                             attention_dropout=attention_dropout,\n",
    "#                             attention_batch_first=attention_batch_first,\n",
    "#                             layer_norm_eps=layer_norm_eps,\n",
    "#                             activation=expert_activation,\n",
    "#                         )\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     layers.append(\n",
    "#                         SparseTransformerLayer(\n",
    "#                             embed_dim=embed_dim,\n",
    "#                             ffn_dim=ffn_dim,\n",
    "#                             num_heads=num_heads,\n",
    "#                             num_experts=num_experts,\n",
    "#                             num_shared_experts=num_shared_experts,\n",
    "#                             top_k=router_top_k,\n",
    "#                             expert_capacity=expert_capacity,\n",
    "#                             expert_activation=expert_activation,\n",
    "#                             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                             attention_dropout=attention_dropout,\n",
    "#                             attention_batch_first=attention_batch_first,\n",
    "#                             layer_norm_eps=layer_norm_eps,\n",
    "#                             router_dtype=router_dtype,\n",
    "#                             router_bias=router_bias,\n",
    "#                             router_jitter=router_jitter,\n",
    "#                             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                             router_class=router_class,\n",
    "#                             expert_class=expert_class,\n",
    "#                         )\n",
    "#                     )\n",
    "#             self.layers = nn.ModuleList(layers)\n",
    "#         # all sparse layers\n",
    "#         else:\n",
    "#             self.layers = nn.ModuleList(\n",
    "#                 [\n",
    "#                     SparseTransformerLayer(\n",
    "#                         embed_dim=embed_dim,\n",
    "#                         ffn_dim=ffn_dim,\n",
    "#                         num_heads=num_heads,\n",
    "#                         num_experts=num_experts,\n",
    "#                         num_shared_experts=num_shared_experts,\n",
    "#                         top_k=router_top_k,\n",
    "#                         expert_capacity=expert_capacity,\n",
    "#                         expert_activation=expert_activation,\n",
    "#                         expert_ffn_dropout=expert_ffn_dropout,\n",
    "#                         attention_dropout=attention_dropout,\n",
    "#                         attention_batch_first=attention_batch_first,\n",
    "#                         layer_norm_eps=layer_norm_eps,\n",
    "#                         router_dtype=router_dtype,\n",
    "#                         router_bias=router_bias,\n",
    "#                         router_jitter=router_jitter,\n",
    "#                         router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#                         router_class=router_class,\n",
    "#                         expert_class=expert_class,\n",
    "#                     )\n",
    "#                     for _ in range(num_layers)\n",
    "#                 ]\n",
    "#             )\n",
    "#         self.embedding_dropout = nn.Dropout(token_embedding_dropout)\n",
    "#         self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.attention_batch_first = attention_batch_first\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = False,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "\n",
    "#         input_ids: torch.LomgTensor\n",
    "#             Tokenized input IDs\n",
    "\n",
    "#         attention_mask: torch.BoolTensor\n",
    "#             Attention mask\n",
    "\n",
    "#         output_attentions: bool\n",
    "#             Whether to output attention weights\n",
    "\n",
    "#         output_hidden_states: bool\n",
    "#             Whether to output hidden states\n",
    "\n",
    "#         output_router_logits: bool\n",
    "#             Whether to output router logits\n",
    "\n",
    "#         return_dict: bool\n",
    "#             Whether to return a dictionary of outputs (returns a tuple by default)\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#         --------\n",
    "#         output (tuple or dict):\n",
    "#             If `return_dict` is ``True``, the output is a ``dict`` of outputs:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - router_z_loss (torch.FloatTensor): router z loss\n",
    "#                 - router_aux_loss (torch.FloatTensor): router auxiliary loss\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#             If `return_dict` is ``False``, the output is a ``tuple`` with the f0llowing elements:\n",
    "#                 - last_hidden_state (torch.FloatTensor): last hidden state\n",
    "#                 - attentions (torch.FloatTensor): attention weights\n",
    "#                 - hidden_states (torch.FloatTensor): hidden states\n",
    "#                 - router_logits (torch.FloatTensor): router logits\n",
    "#         \"\"\"\n",
    "#         # init\n",
    "#         attn_weights = []\n",
    "#         hidden_states = {}\n",
    "#         router_logits = []\n",
    "#         expert_indexes = []\n",
    "\n",
    "#         # embeddings\n",
    "#         x = self.embed_tokens(input_ids)\n",
    "#         x = self.embed_positions(x)\n",
    "#         x = self.embedding_dropout(x)\n",
    "\n",
    "#         # encoder\n",
    "#         for layer_idx, layer in enumerate(self.layers, 1):\n",
    "#             if layer_idx % 2 == 0 or not self.alternate_sparsity:\n",
    "#                 # sparse layer, so we need to collect router/expert info\n",
    "#                 x = layer(\n",
    "#                     x,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     key_padding_mask=key_padding_mask,\n",
    "#                     need_weights=output_attentions,\n",
    "#                     output_router_logits=output_router_logits,\n",
    "#                 )\n",
    "#                 if output_attentions:\n",
    "#                     x, attn, router_tuple = x\n",
    "#                     attn_weights.append(attn)\n",
    "#                 else:\n",
    "#                     x, router_tuple = x\n",
    "#                 router_logits.append(router_tuple[0])\n",
    "#                 expert_indexes.append(router_tuple[1])\n",
    "#                 if output_hidden_states:\n",
    "#                     hidden_states[layer_idx] = x\n",
    "#             else:\n",
    "#                 # dense layer, no router info needed\n",
    "#                 x = layer(\n",
    "#                     x,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     need_weights=output_attentions,\n",
    "#                 )\n",
    "#                 if output_attentions:\n",
    "#                     x, attn = x\n",
    "#                     attn_weights.append(attn)\n",
    "#                 if output_hidden_states:\n",
    "#                     hidden_states[layer_idx] = x\n",
    "#         x = self.final_norm(x)\n",
    "\n",
    "#         # Compute the router losses (z_loss + auxiliary loss)\n",
    "#         cat_router_logits = torch.cat(router_logits, dim=1)\n",
    "#         cat_expert_indexes = torch.cat(expert_indexes, dim=1)\n",
    "#         z_loss = router_z_loss(cat_router_logits)\n",
    "\n",
    "#         # results\n",
    "#         result = MaskedLMOutput(\n",
    "#             last_hidden_state=x,\n",
    "#             router_z_loss=z_loss,\n",
    "#         )\n",
    "#         if output_attentions:\n",
    "#             # attentions: B x L x H x T x T\n",
    "#             attentions = torch.stack(attn_weights, 1)\n",
    "#             attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "#             result[\"attentions\"] = attentions\n",
    "#         if output_hidden_states:\n",
    "#             result[\"hidden_states\"] = hidden_states\n",
    "#         if output_router_logits:\n",
    "#             result[\"router_logits\"] = cat_router_logits\n",
    "#         if output_expert_indices:\n",
    "#             result[\"expert_indices\"] = cat_expert_indexes\n",
    "#         if return_dict:\n",
    "#             return result\n",
    "#         return result.as_tuple()\n",
    "\n",
    "\n",
    "# class BalmExpertChoiceMoEForMaskedLM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     BALM Mixture of Experts model for Masked Language Modeling.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim: int,\n",
    "#         ffn_dim: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         num_experts: int,\n",
    "#         expert_capacity: int,\n",
    "#         vocab_size: int,\n",
    "#         max_length: int = 320,\n",
    "#         num_shared_experts: int = 0,\n",
    "#         expert_activation: str = \"gelu\",\n",
    "#         expert_ffn_dropout: float = 0.0,\n",
    "#         alternate_sparsity: bool = False,\n",
    "#         token_embedding_dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         attention_batch_first: bool = True,\n",
    "#         layer_norm_eps: float = 1e-5,\n",
    "#         router_dtype: str = \"float32\",\n",
    "#         router_top_k: int = 1,\n",
    "#         router_bias: bool = False,\n",
    "#         router_jitter: float = 0.0,\n",
    "#         router_ignore_padding_tokens: bool = True,\n",
    "#         router_z_loss_coef: float = 0.001,\n",
    "#         # router_aux_loss_coef: float = 0.001,\n",
    "#         padding_idx: int = 0,\n",
    "#         router_class: nn.Module = ExpertChoiceRouter,\n",
    "#         expert_class: nn.Module = Expert,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.balm = BalmExpertChoiceMoEModel(\n",
    "#             embed_dim=embed_dim,\n",
    "#             ffn_dim=ffn_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             num_heads=num_heads,\n",
    "#             num_experts=num_experts,\n",
    "#             num_shared_experts=num_shared_experts,\n",
    "#             router_top_k=router_top_k,\n",
    "#             expert_capacity=expert_capacity,\n",
    "#             vocab_size=vocab_size,\n",
    "#             max_length=max_length,\n",
    "#             expert_activation=expert_activation,\n",
    "#             expert_ffn_dropout=expert_ffn_dropout,\n",
    "#             alternate_sparsity=alternate_sparsity,\n",
    "#             token_embedding_dropout=token_embedding_dropout,\n",
    "#             attention_dropout=attention_dropout,\n",
    "#             attention_batch_first=attention_batch_first,\n",
    "#             layer_norm_eps=layer_norm_eps,\n",
    "#             router_dtype=router_dtype,\n",
    "#             router_bias=router_bias,\n",
    "#             router_jitter=router_jitter,\n",
    "#             router_ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "#             padding_idx=padding_idx,\n",
    "#             router_class=router_class,\n",
    "#             expert_class=expert_class,\n",
    "#         )\n",
    "#         self.lm_head = BalmLMHead(\n",
    "#             embed_dim=embed_dim,\n",
    "#             output_dim=vocab_size,\n",
    "#             # weight=self.balm.embed_tokens.weight,\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#         self.router_z_loss_coef = router_z_loss_coef\n",
    "#         # self.router_aux_loss_coef = router_aux_loss_coef\n",
    "\n",
    "#     @property\n",
    "#     def num_parameters(self):\n",
    "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: torch.Tensor,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         key_padding_mask: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: bool = False,\n",
    "#         output_hidden_states: bool = False,\n",
    "#         output_router_logits: bool = True,\n",
    "#         output_expert_indices: bool = False,\n",
    "#         return_dict: bool = True,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids (torch.LongTensor): tokenized input IDs\n",
    "#             attention_mask (torch.BoolTensor): attention mask\n",
    "#             return_dict (bool): return a dictionary of outputs\n",
    "#         \"\"\"\n",
    "#         # encoder\n",
    "#         outputs = self.balm(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             key_padding_mask=key_padding_mask,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             output_router_logits=output_router_logits,\n",
    "#             output_expert_indices=output_expert_indices,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         x = outputs[\"last_hidden_state\"]\n",
    "#         router_z_loss = outputs[\"router_z_loss\"]\n",
    "#         # router_aux_loss = outputs[\"router_aux_loss\"]\n",
    "\n",
    "#         # LM head\n",
    "#         lm_logits = self.lm_head(x)\n",
    "#         outputs[\"logits\"] = lm_logits\n",
    "\n",
    "#         # loss\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             loss = self.criterion(\n",
    "#                 lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1)\n",
    "#             )\n",
    "#             outputs[\"lm_loss\"] = loss\n",
    "\n",
    "#             if output_router_logits:\n",
    "#                 z_loss = self.router_z_loss_coef * (router_z_loss)\n",
    "#                 #     aux_loss = self.router_aux_loss_coef * (router_aux_loss)\n",
    "#                 outputs[\"router_z_loss\"] = z_loss\n",
    "#                 #     outputs[\"router_aux_loss\"] = aux_loss\n",
    "#                 loss = loss + z_loss\n",
    "#             outputs[\"loss\"] = loss\n",
    "\n",
    "#         if return_dict:\n",
    "#             return outputs\n",
    "#         return outputs.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c949328d9b854c42b629e246316ed947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a4dd794a244536b139958cafd83728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061c780cc25545ad858ffb659eb18e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    remove_columns=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BalmMoERoPEForMaskedLM(\n",
    "model = BalmExpertChoiceMoEForMaskedLM(\n",
    "    embed_dim=256,\n",
    "    ffn_dim=1024,\n",
    "    num_experts=4,\n",
    "    num_shared_experts=0,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    alternate_sparsity=True,\n",
    "    # router_top_k=1,\n",
    "    # router_class=ExpertChoiceRouter,\n",
    "    expert_capacity=128,\n",
    "    # expert_capacity=128,\n",
    "    router_z_loss_coef=0.01,\n",
    "    # router_aux_loss_coef=0.01,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12692257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    epochs=1,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    warmup_steps=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    # per_device_eval_batch_size=32,\n",
    "    use_cpu=True,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd338d4ea78249b3a4b9d2de8ab6cd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10   | loss: 2.9767 | lm_loss: 2.9582 | router_z_loss: 0.0186 | lr: 0.000080\n",
      "step 20   | loss: 2.7883 | lm_loss: 2.7792 | router_z_loss: 0.0091 | lr: 0.000160\n",
      "step 30   | loss: 2.6481 | lm_loss: 2.6438 | router_z_loss: 0.0043 | lr: 0.000240\n",
      "step 40   | loss: 2.4974 | lm_loss: 2.4945 | router_z_loss: 0.0029 | lr: 0.000320\n",
      "step 50   | loss: 2.3530 | lm_loss: 2.3511 | router_z_loss: 0.0020 | lr: 0.000400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0a1de92e364df6bcc96f77436d5767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 2.3451\n",
      "step 60   | loss: 2.2613 | lm_loss: 2.2595 | router_z_loss: 0.0018 | lr: 0.000398\n",
      "step 70   | loss: 2.0887 | lm_loss: 2.0874 | router_z_loss: 0.0014 | lr: 0.000396\n",
      "step 80   | loss: 2.1843 | lm_loss: 2.1834 | router_z_loss: 0.0010 | lr: 0.000394\n",
      "step 90   | loss: 2.0637 | lm_loss: 2.0629 | router_z_loss: 0.0008 | lr: 0.000392\n",
      "step 100  | loss: 2.0528 | lm_loss: 2.0520 | router_z_loss: 0.0008 | lr: 0.000390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac187397428d418c9d497011ecd3dde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 2.0233\n",
      "step 110  | loss: 1.9447 | lm_loss: 1.9440 | router_z_loss: 0.0007 | lr: 0.000388\n",
      "step 120  | loss: 1.9800 | lm_loss: 1.9795 | router_z_loss: 0.0005 | lr: 0.000386\n",
      "step 130  | loss: 1.9596 | lm_loss: 1.9588 | router_z_loss: 0.0008 | lr: 0.000384\n",
      "step 140  | loss: 1.9560 | lm_loss: 1.9554 | router_z_loss: 0.0006 | lr: 0.000382\n",
      "step 150  | loss: 1.9410 | lm_loss: 1.9405 | router_z_loss: 0.0005 | lr: 0.000380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fca5ffa9bc40fd981df48ce7d7d5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.9479\n",
      "step 160  | loss: 2.0627 | lm_loss: 2.0620 | router_z_loss: 0.0007 | lr: 0.000378\n",
      "step 170  | loss: 1.9097 | lm_loss: 1.9093 | router_z_loss: 0.0004 | lr: 0.000376\n",
      "step 180  | loss: 1.9222 | lm_loss: 1.9216 | router_z_loss: 0.0005 | lr: 0.000374\n",
      "step 190  | loss: 1.9481 | lm_loss: 1.9476 | router_z_loss: 0.0005 | lr: 0.000373\n",
      "step 200  | loss: 1.8776 | lm_loss: 1.8772 | router_z_loss: 0.0004 | lr: 0.000371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b05eb2eaf17468897d61c1d469b04ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.8687\n",
      "step 210  | loss: 1.8475 | lm_loss: 1.8470 | router_z_loss: 0.0005 | lr: 0.000369\n",
      "step 220  | loss: 1.8534 | lm_loss: 1.8528 | router_z_loss: 0.0005 | lr: 0.000367\n",
      "step 230  | loss: 1.7946 | lm_loss: 1.7941 | router_z_loss: 0.0005 | lr: 0.000365\n",
      "step 240  | loss: 1.8529 | lm_loss: 1.8525 | router_z_loss: 0.0004 | lr: 0.000363\n",
      "step 250  | loss: 1.8125 | lm_loss: 1.8118 | router_z_loss: 0.0007 | lr: 0.000361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c7527c4a804d2e828f65b383a297a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.7622\n",
      "step 260  | loss: 1.7047 | lm_loss: 1.7043 | router_z_loss: 0.0004 | lr: 0.000359\n",
      "step 270  | loss: 1.7783 | lm_loss: 1.7777 | router_z_loss: 0.0006 | lr: 0.000357\n",
      "step 280  | loss: 1.5829 | lm_loss: 1.5824 | router_z_loss: 0.0005 | lr: 0.000355\n",
      "step 290  | loss: 1.6948 | lm_loss: 1.6942 | router_z_loss: 0.0006 | lr: 0.000353\n",
      "step 300  | loss: 1.6742 | lm_loss: 1.6736 | router_z_loss: 0.0006 | lr: 0.000351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d350384f9460cbc10a57a981af9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.6288\n",
      "step 310  | loss: 1.5057 | lm_loss: 1.5052 | router_z_loss: 0.0005 | lr: 0.000349\n",
      "step 320  | loss: 1.5582 | lm_loss: 1.5577 | router_z_loss: 0.0005 | lr: 0.000347\n",
      "step 330  | loss: 1.5406 | lm_loss: 1.5401 | router_z_loss: 0.0005 | lr: 0.000345\n",
      "step 340  | loss: 1.5867 | lm_loss: 1.5862 | router_z_loss: 0.0005 | lr: 0.000343\n",
      "step 350  | loss: 1.5545 | lm_loss: 1.5540 | router_z_loss: 0.0005 | lr: 0.000341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c073cc37218741f9987d7316f2c73a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.4725\n",
      "step 360  | loss: 1.4632 | lm_loss: 1.4628 | router_z_loss: 0.0004 | lr: 0.000339\n",
      "step 370  | loss: 1.4236 | lm_loss: 1.4232 | router_z_loss: 0.0004 | lr: 0.000337\n",
      "step 380  | loss: 1.4277 | lm_loss: 1.4272 | router_z_loss: 0.0005 | lr: 0.000335\n",
      "step 390  | loss: 1.4391 | lm_loss: 1.4388 | router_z_loss: 0.0004 | lr: 0.000333\n",
      "step 400  | loss: 1.3785 | lm_loss: 1.3781 | router_z_loss: 0.0004 | lr: 0.000331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b757fb1464af59cef06b3e350d1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< EVAL >>> loss: 1.3424\n",
      "step 410  | loss: 1.2435 | lm_loss: 1.2431 | router_z_loss: 0.0004 | lr: 0.000329\n",
      "step 420  | loss: 1.2931 | lm_loss: 1.2927 | router_z_loss: 0.0005 | lr: 0.000327\n",
      "step 430  | loss: 1.2652 | lm_loss: 1.2648 | router_z_loss: 0.0004 | lr: 0.000325\n",
      "step 440  | loss: 1.1915 | lm_loss: 1.1911 | router_z_loss: 0.0004 | lr: 0.000323\n",
      "step 450  | loss: 1.3268 | lm_loss: 1.3264 | router_z_loss: 0.0005 | lr: 0.000321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb16f0156b647dea36e0cae62a056b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/git/BALM/balm/training/trainer.py:223\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# eval\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m completed_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    221\u001b[0m ):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# print(\"Evaluating\")\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m completed_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    229\u001b[0m ):\n",
      "File \u001b[0;32m~/git/BALM/balm/training/trainer.py:274\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, compute_metrics)\u001b[0m\n\u001b[1;32m    272\u001b[0m collated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator(batch)\n\u001b[1;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_inputs(collated)\n\u001b[0;32m--> 274\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    275\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    276\u001b[0m     labels\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    277\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    278\u001b[0m     key_padding_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m tmp_eval_loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    281\u001b[0m eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tmp_eval_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 329\u001b[0m, in \u001b[0;36mBalmExpertChoiceMoEForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, key_padding_mask, labels, output_attentions, output_hidden_states, output_router_logits, output_expert_indices, return_dict)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    input_ids (torch.LongTensor): tokenized input IDs\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    attention_mask (torch.BoolTensor): attention mask\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    return_dict (bool): return a dictionary of outputs\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# encoder\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalm(\n\u001b[1;32m    330\u001b[0m     input_ids,\n\u001b[1;32m    331\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    332\u001b[0m     key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    333\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    334\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    335\u001b[0m     output_router_logits\u001b[38;5;241m=\u001b[39moutput_router_logits,\n\u001b[1;32m    336\u001b[0m     output_expert_indices\u001b[38;5;241m=\u001b[39moutput_expert_indices,\n\u001b[1;32m    337\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    339\u001b[0m x \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    340\u001b[0m router_z_loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouter_z_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 179\u001b[0m, in \u001b[0;36mBalmExpertChoiceMoEModel.forward\u001b[0;34m(self, input_ids, attention_mask, key_padding_mask, output_attentions, output_hidden_states, output_router_logits, output_expert_indices, return_dict)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malternate_sparsity:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# sparse layer, so we need to collect router/expert info\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    180\u001b[0m             x,\n\u001b[1;32m    181\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    182\u001b[0m             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    183\u001b[0m             need_weights\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    184\u001b[0m             output_router_logits\u001b[38;5;241m=\u001b[39moutput_router_logits,\n\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    187\u001b[0m             x, attn, router_tuple \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 130\u001b[0m, in \u001b[0;36mSparseTransformerLayer.forward\u001b[0;34m(self, x, attention_mask, key_padding_mask, need_weights, output_router_logits)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# attention\u001b[39;00m\n\u001b[1;32m    129\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 130\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    131\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    132\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    133\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    134\u001b[0m     key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    135\u001b[0m     need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m    136\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_weights:\n\u001b[1;32m    139\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/nn/modules/activation.py:1196\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_masks(attn_mask, key_padding_mask, query)\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_native_multi_head_attention(\n\u001b[1;32m   1197\u001b[0m                 query,\n\u001b[1;32m   1198\u001b[0m                 key,\n\u001b[1;32m   1199\u001b[0m                 value,\n\u001b[1;32m   1200\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1201\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1202\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1203\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1204\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1205\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1206\u001b[0m                 merged_mask,\n\u001b[1;32m   1207\u001b[0m                 need_weights,\n\u001b[1;32m   1208\u001b[0m                 average_attn_weights,\n\u001b[1;32m   1209\u001b[0m                 mask_type)\n\u001b[1;32m   1211\u001b[0m any_nested \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_nested\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1213\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
