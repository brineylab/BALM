{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for routers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_experts: int,\n",
    "        expert_capacity: int,\n",
    "        dtype: str = \"float32\",\n",
    "        bias: bool = False,\n",
    "        jitter: float = 0.0,\n",
    "        num_routable_experts: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        self.dtype = getattr(torch, dtype)\n",
    "        self.bias = bias\n",
    "        self.jitter = jitter\n",
    "        self.classifier = nn.Linear(\n",
    "            self.embed_dim,\n",
    "            num_routable_experts\n",
    "            if num_routable_experts is not None\n",
    "            else self.num_experts,\n",
    "            bias=self.bias,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_router_probabilities(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        dim: int = -1,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes router probabilities from input hidden states.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, hidden_dim) from which\n",
    "            router probabilities are computed.\n",
    "\n",
    "        dim : int, optional\n",
    "            Dimension along which to compute the softmax. The default is -1, which corresponds\n",
    "            to token-choice routing. For expert choice routing, this should be -2.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        router_probabilities : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, num_experts) corresponding to\n",
    "            the probabilities for each token and expert. Used for routing tokens to experts.\n",
    "\n",
    "        router_logits : torch.Tensor\n",
    "            Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding\n",
    "            to raw router logits. This is used for computing router z-loss.\n",
    "        \"\"\"\n",
    "        # float32 is used to ensure stability. See the discussion of \"selective precision\" in\n",
    "        # https://arxiv.org/abs/2101.03961.\n",
    "        # we also store the input dtype so we can cast the output back to the original dtype\n",
    "        self.input_dtype = x.dtype\n",
    "        x = x.to(self.dtype)\n",
    "        if self.jitter > 0:\n",
    "            x *= torch.empty_like(x).uniform_(1.0 - self.jitter, 1.0 + self.jitter)\n",
    "\n",
    "        # shape: [batch_size, sequence_length, num_experts]\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        # apply softmax and cast back to the original dtype\n",
    "        probabilities = F.softmax(logits, dim=dim, dtype=self.dtype).to(\n",
    "            self.input_dtype\n",
    "        )\n",
    "        return probabilities, logits\n",
    "\n",
    "\n",
    "class TopKRouter(RouterBase):\n",
    "    \"\"\"\n",
    "    This router uses the \"token choice of top-k experts\" strategy. For example, if k=1, this\n",
    "    replicates the top-1 routing strategy introduced in the `Switch Transformers`_ paper.\n",
    "    Alternatively, if k=2, this replicates the top-2 routing strategy introduced in the `GShard`_\n",
    "    paper. Tokens are routed to their expert of choice until the expert's `expert_capacity` is\n",
    "    reached. Shared experts, which process all tokens, are implemented as described in the\n",
    "    `DeepSeqMoE`_ paper.\n",
    "\n",
    "    .. note::\n",
    "        There is no guarantee that each token will be processed by an expert,\n",
    "        or that every expert will receive at least one token.\n",
    "\n",
    "    If tokens are routed to an expert which is above capacity, they are not processed by any expert\n",
    "    and their hidden states are passed to the subsequent layer unchanged.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    num_experts : int\n",
    "        Number of experts.\n",
    "\n",
    "    expert_capacity : int\n",
    "        Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "    top_k : int, optional\n",
    "        Number of top experts to route each token to. The default is 1.\n",
    "\n",
    "    num_shared_experts : int, optional\n",
    "        Number of shared experts that process all tokens. The default is 0.\n",
    "\n",
    "    dtype : str, optional\n",
    "        Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "    bias : bool, optional\n",
    "        Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "    jitter : float, optional\n",
    "        Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "    ignore_padding_tokens : bool, optional\n",
    "        Whether to ignore padding tokens when computing router probabilities.\n",
    "        The default is ``True``.\n",
    "\n",
    "\n",
    "    .. _Switch Transformers:\n",
    "        https://arxiv.org/abs/2101.03961\n",
    "\n",
    "    .. _GShard:\n",
    "        https://arxiv.org/abs/2006.16668\n",
    "\n",
    "    .. _DeepSeqMoE:\n",
    "        https://arxiv.org/abs/2401.06066\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_experts: int,\n",
    "        expert_capacity: int,\n",
    "        top_k: int = 1,\n",
    "        num_shared_experts: int = 0,\n",
    "        dtype: str = \"float32\",\n",
    "        bias: bool = False,\n",
    "        jitter: float = 0.0,\n",
    "        ignore_padding_tokens: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            embed_dim=embed_dim,\n",
    "            num_experts=num_experts,\n",
    "            expert_capacity=expert_capacity,\n",
    "            dtype=dtype,\n",
    "            bias=bias,\n",
    "            jitter=jitter,\n",
    "            num_routable_experts=num_experts - num_shared_experts,\n",
    "        )\n",
    "        self.top_k = top_k\n",
    "        self.num_shared_experts = num_shared_experts\n",
    "        self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Token choice of top-k experts, with optional shared experts processing all tokens.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        expert_mask : torch.Tensor\n",
    "            Binary mask tensor of shape (batch_size, sequence_length, num_experts)\n",
    "            indicating which experts the token should be routed to (including shared experts).\n",
    "\n",
    "        router_probabilities : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "            the router probabilities.\n",
    "\n",
    "        router_logits : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "            the router logits.\n",
    "        \"\"\"\n",
    "        num_routable_experts = self.num_experts - self.num_shared_experts\n",
    "\n",
    "        # router\n",
    "        router_probs, router_logits = self._compute_router_probabilities(x)\n",
    "        topk_values, topk_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "        expert_mask = F.one_hot(topk_indices, num_classes=num_routable_experts).sum(dim=-2)\n",
    "\n",
    "        # mask tokens if their desired experts are above capacity\n",
    "        token_priority = torch.cumsum(expert_mask, dim=-2)\n",
    "        expert_capacity_mask = token_priority <= self.expert_capacity\n",
    "        expert_mask = expert_mask * expert_capacity_mask\n",
    "\n",
    "        # get the probabilities of the top-choice experts for each token\n",
    "        # router_probs = expert_values * expert_mask\n",
    "\n",
    "        # Add shared experts processing all tokens\n",
    "        if self.num_shared_experts > 0:\n",
    "            shared_expert_mask = torch.ones_like(\n",
    "                router_probs[..., : self.num_shared_experts]\n",
    "            )\n",
    "            expert_mask = torch.cat((shared_expert_mask, expert_mask), dim=-1)\n",
    "\n",
    "        return expert_mask, router_probs, router_logits\n",
    "\n",
    "\n",
    "class ExpertChoiceRouter(RouterBase):\n",
    "    \"\"\"\n",
    "    This router uses the \"expert choice of top-k tokens\" strategy, as originally described\n",
    "    in the `Mixture-of-Experts with Expert Choice Routing`_ paper. This automatically\n",
    "    balances the number of tokens processed by each expert, and eliminates the\n",
    "    need for an auxiliary (load-balancing) router loss.\n",
    "\n",
    "    .. note::\n",
    "        There is no guarantee that each token will be processed by an expert. In fact,\n",
    "        one of the primary benefits of expert choice routing is thought to be their\n",
    "        ability to heterogeneously devote computation to a subset of highly complex/difficult\n",
    "        tokens.\n",
    "\n",
    "    If tokens are not selected by an expert, their hidden states are passed to the\n",
    "    subsequent layer unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    num_experts : int\n",
    "        Number of experts.\n",
    "\n",
    "    expert_capacity : int\n",
    "        Maximum number of tokens that can be routed to each expert.\n",
    "\n",
    "    num_shared_experts : int, optional\n",
    "        Number of shared experts that process all tokens. The default is 0.\n",
    "\n",
    "    dtype : str, optional\n",
    "        Data type to use for router probabilities. The default is \"float32\".\n",
    "\n",
    "    bias : bool, optional\n",
    "        Whether to add bias to the router classifier. The default is ``False``.\n",
    "\n",
    "    jitter : float, optional\n",
    "        Amount of jitter to add to the router probabilities. The default is ``0.0``.\n",
    "\n",
    "    ignore_padding_tokens : bool, optional\n",
    "        Whether to ignore padding tokens when computing router probabilities.\n",
    "        The default is ``True``.\n",
    "\n",
    "    .. _Mixture-of-Experts with Expert Choice Routing:\n",
    "        https://arxiv.org/abs/2202.09368\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_experts: int,\n",
    "        expert_capacity: int,\n",
    "        num_shared_experts: int = 0,\n",
    "        dtype: str = \"float32\",\n",
    "        bias: bool = False,\n",
    "        jitter: float = 0.0,\n",
    "        ignore_padding_tokens: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            embed_dim=embed_dim,\n",
    "            num_experts=num_experts,\n",
    "            expert_capacity=expert_capacity,\n",
    "            dtype=dtype,\n",
    "            bias=bias,\n",
    "            jitter=jitter,\n",
    "            num_routable_experts=num_experts - num_shared_experts,\n",
    "        )\n",
    "        self.num_shared_experts = num_shared_experts\n",
    "        self.ignore_padding_tokens = ignore_padding_tokens\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Expert choice of top-k tokens, with optional shared experts that process all tokens.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        expert_mask : torch.Tensor\n",
    "            Binary mask tensor of shape (batch_size, sequence_length, num_experts) indicating\n",
    "            which tokens are selected for each expert and which are processed by shared experts.\n",
    "\n",
    "        router_probabilities : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "            the router probabilities.\n",
    "\n",
    "        router_logits : torch.Tensor\n",
    "            Tensor of shape (batch_size, sequence_length, num_experts) containing\n",
    "            the router logits.\n",
    "        \"\"\"\n",
    "        router_probs, router_logits = self._compute_router_probabilities(x, dim=1)\n",
    "        expert_mask = torch.zeros_like(router_probs)\n",
    "\n",
    "        # Select top-k tokens for each expert\n",
    "        for i in range(self.num_experts - self.num_shared_experts):\n",
    "            _, top_k_indices = torch.topk(\n",
    "                router_probs[..., i], k=self.expert_capacity, dim=1\n",
    "            )\n",
    "            expert_mask[:, :, i].scatter_(1, top_k_indices, 1)\n",
    "\n",
    "        # Ensure that the mask is binary\n",
    "        # expert_mask = expert_mask.clamp(max=1)\n",
    "\n",
    "        # Add shared experts processing all tokens\n",
    "        if self.num_shared_experts > 0:\n",
    "            shared_expert_mask = torch.ones_like(\n",
    "                router_probs[..., : self.num_shared_experts]\n",
    "            )\n",
    "            expert_mask = torch.cat((shared_expert_mask, expert_mask), dim=-1)\n",
    "\n",
    "        return expert_mask, router_probs, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = TopKRouter(\n",
    "    embed_dim=1024, \n",
    "    num_experts=16,\n",
    "    expert_capacity=8, \n",
    "    # top_k=2, \n",
    "    num_shared_experts=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(10, 100, 1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_mask, router_probs, router_logits = router(data)\n",
    "\n",
    "# print(expert_mask.shape)\n",
    "# print(router_probs.shape)\n",
    "# print(router_logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_mask[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0074, 0.0060, 0.0094, 0.0123, 0.0191, 0.0078, 0.0176, 0.0061, 0.0032,\n",
       "        0.0068, 0.0101, 0.0066, 0.0323, 0.0079, 0.0119],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_probs[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1264, -0.5073, -0.0097,  0.4879,  0.7862, -0.1197,  0.8284, -0.3132,\n",
       "        -0.9528, -0.3334,  0.2162, -0.2432,  1.4168, -0.1742,  0.3891],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_logits[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 16])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 3., 2., 2., 2., 4., 3., 1., 5., 2., 2., 3., 3., 4., 3., 2.,\n",
       "         2., 1., 3., 2., 4., 2., 1., 2., 2., 2., 2., 3., 1., 3., 2., 1., 2., 4.,\n",
       "         4., 3., 2., 2., 3., 1., 4., 2., 3., 3., 3., 1., 1., 2., 3., 3., 2., 1.,\n",
       "         2., 2., 3., 1., 4., 4., 1., 3., 2., 2., 2., 2., 1., 1., 2., 1., 3., 3.,\n",
       "         2., 2., 3., 2., 3., 3., 2., 1., 1., 2., 2., 2., 2., 2., 1., 1., 1., 2.,\n",
       "         2., 1., 3., 1., 3., 1., 2., 1., 1., 3.],\n",
       "        [1., 3., 2., 4., 2., 2., 3., 1., 3., 2., 3., 4., 3., 3., 1., 1., 3., 1.,\n",
       "         2., 3., 1., 1., 2., 4., 2., 3., 1., 1., 3., 2., 3., 3., 3., 4., 2., 1.,\n",
       "         1., 6., 2., 3., 3., 2., 1., 1., 1., 1., 3., 3., 3., 3., 1., 3., 4., 1.,\n",
       "         3., 3., 1., 4., 2., 2., 3., 2., 2., 1., 1., 2., 1., 3., 3., 1., 2., 1.,\n",
       "         2., 3., 1., 1., 2., 4., 2., 2., 2., 2., 1., 1., 2., 2., 1., 2., 1., 2.,\n",
       "         3., 3., 3., 2., 3., 3., 2., 3., 1., 2.],\n",
       "        [1., 4., 2., 2., 1., 1., 2., 2., 2., 3., 3., 2., 2., 2., 2., 1., 3., 2.,\n",
       "         1., 2., 4., 4., 5., 2., 1., 2., 1., 4., 1., 2., 2., 1., 3., 1., 3., 1.,\n",
       "         3., 4., 7., 2., 2., 2., 2., 1., 3., 3., 2., 2., 2., 3., 2., 2., 3., 1.,\n",
       "         2., 1., 3., 2., 3., 1., 2., 3., 2., 2., 2., 2., 3., 2., 1., 1., 4., 1.,\n",
       "         1., 2., 2., 2., 2., 2., 4., 3., 2., 2., 3., 2., 4., 3., 1., 1., 1., 1.,\n",
       "         1., 2., 1., 1., 2., 2., 2., 4., 3., 4.],\n",
       "        [1., 2., 2., 4., 2., 2., 3., 1., 3., 1., 3., 1., 3., 1., 2., 4., 3., 2.,\n",
       "         3., 3., 3., 1., 1., 3., 4., 1., 4., 1., 1., 1., 2., 2., 4., 2., 3., 1.,\n",
       "         4., 1., 3., 2., 1., 1., 1., 2., 2., 2., 2., 1., 5., 4., 2., 1., 1., 2.,\n",
       "         4., 1., 3., 4., 2., 5., 1., 2., 2., 2., 1., 3., 4., 2., 2., 1., 2., 4.,\n",
       "         1., 2., 1., 1., 1., 1., 1., 3., 1., 4., 2., 3., 3., 2., 3., 1., 1., 4.,\n",
       "         2., 3., 3., 1., 2., 4., 2., 2., 3., 1.],\n",
       "        [2., 2., 2., 2., 2., 1., 3., 1., 2., 2., 2., 2., 1., 1., 2., 3., 1., 3.,\n",
       "         3., 2., 3., 3., 3., 3., 2., 2., 2., 3., 2., 1., 4., 1., 3., 3., 3., 3.,\n",
       "         4., 2., 1., 2., 1., 4., 3., 4., 2., 1., 2., 2., 3., 2., 2., 4., 4., 3.,\n",
       "         2., 2., 1., 1., 3., 2., 1., 3., 2., 2., 1., 5., 3., 6., 2., 3., 1., 3.,\n",
       "         1., 1., 2., 2., 1., 1., 2., 1., 2., 2., 3., 2., 2., 3., 1., 2., 1., 2.,\n",
       "         1., 1., 2., 1., 3., 2., 3., 4., 1., 2.],\n",
       "        [4., 1., 2., 1., 3., 3., 2., 3., 2., 2., 1., 2., 3., 3., 3., 2., 3., 2.,\n",
       "         2., 1., 2., 1., 3., 3., 2., 3., 2., 3., 1., 3., 2., 2., 1., 4., 3., 1.,\n",
       "         2., 2., 2., 4., 3., 1., 2., 1., 1., 2., 1., 2., 5., 3., 1., 2., 1., 3.,\n",
       "         1., 1., 2., 3., 2., 1., 1., 3., 1., 2., 3., 2., 2., 4., 2., 4., 2., 2.,\n",
       "         1., 3., 2., 2., 3., 2., 1., 1., 1., 3., 2., 4., 3., 3., 4., 1., 1., 1.,\n",
       "         2., 4., 2., 1., 2., 1., 4., 2., 4., 3.],\n",
       "        [2., 1., 2., 2., 3., 3., 3., 2., 2., 2., 1., 2., 5., 1., 2., 2., 2., 3.,\n",
       "         2., 4., 4., 3., 1., 3., 1., 1., 3., 3., 1., 3., 3., 1., 2., 2., 2., 2.,\n",
       "         1., 2., 2., 3., 2., 1., 1., 1., 2., 2., 2., 2., 3., 2., 2., 2., 1., 3.,\n",
       "         2., 2., 3., 4., 3., 5., 1., 2., 2., 5., 3., 2., 1., 2., 3., 2., 2., 3.,\n",
       "         3., 2., 2., 2., 2., 1., 2., 2., 1., 2., 1., 3., 3., 2., 2., 5., 3., 1.,\n",
       "         1., 1., 3., 3., 2., 1., 2., 1., 1., 4.],\n",
       "        [1., 3., 2., 1., 2., 3., 3., 2., 5., 1., 3., 3., 5., 2., 4., 2., 5., 2.,\n",
       "         2., 2., 3., 2., 1., 1., 3., 1., 4., 1., 1., 3., 2., 3., 3., 3., 1., 1.,\n",
       "         1., 1., 2., 3., 2., 4., 2., 1., 4., 1., 1., 2., 4., 3., 3., 2., 1., 1.,\n",
       "         1., 3., 3., 3., 1., 1., 1., 2., 3., 3., 3., 2., 1., 3., 1., 5., 3., 1.,\n",
       "         1., 2., 2., 2., 2., 3., 1., 1., 1., 3., 2., 1., 2., 1., 3., 2., 4., 2.,\n",
       "         1., 1., 3., 3., 2., 2., 3., 4., 1., 1.],\n",
       "        [1., 2., 1., 3., 1., 3., 3., 3., 2., 3., 1., 4., 2., 1., 2., 1., 1., 3.,\n",
       "         1., 1., 3., 4., 2., 2., 3., 4., 3., 1., 3., 2., 2., 3., 2., 4., 2., 1.,\n",
       "         2., 1., 4., 2., 2., 2., 4., 2., 2., 3., 2., 3., 4., 2., 2., 1., 1., 3.,\n",
       "         4., 4., 1., 1., 2., 1., 3., 2., 2., 2., 2., 3., 1., 2., 3., 2., 4., 2.,\n",
       "         1., 1., 1., 1., 2., 2., 1., 2., 2., 3., 2., 3., 2., 1., 2., 4., 2., 4.,\n",
       "         3., 1., 2., 1., 2., 2., 1., 1., 4., 4.],\n",
       "        [1., 2., 1., 2., 2., 2., 2., 3., 2., 1., 2., 1., 3., 1., 2., 1., 5., 2.,\n",
       "         2., 2., 3., 1., 3., 1., 5., 1., 1., 1., 1., 3., 1., 2., 4., 2., 1., 2.,\n",
       "         4., 3., 2., 3., 2., 4., 3., 2., 2., 3., 2., 1., 4., 1., 2., 3., 4., 4.,\n",
       "         2., 5., 3., 2., 2., 2., 1., 1., 4., 3., 3., 1., 3., 3., 1., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 3., 5., 1., 1., 2., 2., 1., 2., 3., 1., 1., 1., 3., 2.,\n",
       "         2., 1., 2., 3., 2., 2., 2., 3., 2., 3.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_mask.sum(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 2., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.tensor([2, 6], dtype=torch.long)\n",
    "values = torch.tensor([1, 2], dtype=torch.float32)\n",
    "\n",
    "x = torch.zeros(10)\n",
    "x[indices] = values\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
