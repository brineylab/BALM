{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Union, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from balm.config import BalmConfig\n",
    "\n",
    "from balm.models import BalmForMaskedLM, BalmForSequenceClassification\n",
    "from balm.models.base import BalmBase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BalmConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "balm = BalmForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['balm', 'lm_head', 'criterion'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balm._modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "balm_classifier = BalmForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['balm', 'classifier', 'criterion'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balm_classifier._modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(balm, \"module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balm.embed_tokens.weight\n",
      "balm.layers.0.norm1.weight\n",
      "balm.layers.0.norm1.bias\n",
      "balm.layers.0.norm2.weight\n",
      "balm.layers.0.norm2.bias\n",
      "balm.layers.0.attention.in_proj_weight\n",
      "balm.layers.0.attention.in_proj_bias\n",
      "balm.layers.0.attention.out_proj.weight\n",
      "balm.layers.0.attention.out_proj.bias\n",
      "balm.layers.0.feed_forward.0.weight\n",
      "balm.layers.0.feed_forward.0.bias\n",
      "balm.layers.0.feed_forward.2.weight\n",
      "balm.layers.0.feed_forward.2.bias\n",
      "balm.layers.1.norm1.weight\n",
      "balm.layers.1.norm1.bias\n",
      "balm.layers.1.norm2.weight\n",
      "balm.layers.1.norm2.bias\n",
      "balm.layers.1.attention.in_proj_weight\n",
      "balm.layers.1.attention.in_proj_bias\n",
      "balm.layers.1.attention.out_proj.weight\n",
      "balm.layers.1.attention.out_proj.bias\n",
      "balm.layers.1.feed_forward.0.weight\n",
      "balm.layers.1.feed_forward.0.bias\n",
      "balm.layers.1.feed_forward.2.weight\n",
      "balm.layers.1.feed_forward.2.bias\n",
      "balm.layers.2.norm1.weight\n",
      "balm.layers.2.norm1.bias\n",
      "balm.layers.2.norm2.weight\n",
      "balm.layers.2.norm2.bias\n",
      "balm.layers.2.attention.in_proj_weight\n",
      "balm.layers.2.attention.in_proj_bias\n",
      "balm.layers.2.attention.out_proj.weight\n",
      "balm.layers.2.attention.out_proj.bias\n",
      "balm.layers.2.feed_forward.0.weight\n",
      "balm.layers.2.feed_forward.0.bias\n",
      "balm.layers.2.feed_forward.2.weight\n",
      "balm.layers.2.feed_forward.2.bias\n",
      "balm.layers.3.norm1.weight\n",
      "balm.layers.3.norm1.bias\n",
      "balm.layers.3.norm2.weight\n",
      "balm.layers.3.norm2.bias\n",
      "balm.layers.3.attention.in_proj_weight\n",
      "balm.layers.3.attention.in_proj_bias\n",
      "balm.layers.3.attention.out_proj.weight\n",
      "balm.layers.3.attention.out_proj.bias\n",
      "balm.layers.3.feed_forward.0.weight\n",
      "balm.layers.3.feed_forward.0.bias\n",
      "balm.layers.3.feed_forward.2.weight\n",
      "balm.layers.3.feed_forward.2.bias\n",
      "balm.layers.4.norm1.weight\n",
      "balm.layers.4.norm1.bias\n",
      "balm.layers.4.norm2.weight\n",
      "balm.layers.4.norm2.bias\n",
      "balm.layers.4.attention.in_proj_weight\n",
      "balm.layers.4.attention.in_proj_bias\n",
      "balm.layers.4.attention.out_proj.weight\n",
      "balm.layers.4.attention.out_proj.bias\n",
      "balm.layers.4.feed_forward.0.weight\n",
      "balm.layers.4.feed_forward.0.bias\n",
      "balm.layers.4.feed_forward.2.weight\n",
      "balm.layers.4.feed_forward.2.bias\n",
      "balm.layers.5.norm1.weight\n",
      "balm.layers.5.norm1.bias\n",
      "balm.layers.5.norm2.weight\n",
      "balm.layers.5.norm2.bias\n",
      "balm.layers.5.attention.in_proj_weight\n",
      "balm.layers.5.attention.in_proj_bias\n",
      "balm.layers.5.attention.out_proj.weight\n",
      "balm.layers.5.attention.out_proj.bias\n",
      "balm.layers.5.feed_forward.0.weight\n",
      "balm.layers.5.feed_forward.0.bias\n",
      "balm.layers.5.feed_forward.2.weight\n",
      "balm.layers.5.feed_forward.2.bias\n",
      "balm.final_layer_norm.weight\n",
      "balm.final_layer_norm.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(balm.state_dict().keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_byte_size(dtype):\n",
    "    \"\"\"\n",
    "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    >>> dtype_byte_size(torch.float32)\n",
    "    4\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if dtype == torch.bool:\n",
    "        return 1 / 8\n",
    "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
    "    if bit_search is None:\n",
    "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
    "    bit_size = int(bit_search.groups()[0])\n",
    "    return bit_size // 8\n",
    "\n",
    "\n",
    "def convert_file_size_to_int(size: Union[int, str]):\n",
    "    \"\"\"\n",
    "    Converts a size expressed as a string with digits an unit (like `\"5MB\"`) to an integer (in bytes).\n",
    "\n",
    "    Args:\n",
    "        size (`int` or `str`): The size to convert. Will be directly returned if an `int`.\n",
    "\n",
    "    Example:\n",
    "    ```py\n",
    "    >>> convert_file_size_to_int(\"1MiB\")\n",
    "    1048576\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if isinstance(size, int):\n",
    "        return size\n",
    "    if size.upper().endswith(\"GIB\"):\n",
    "        return int(size[:-3]) * (2**30)\n",
    "    if size.upper().endswith(\"MIB\"):\n",
    "        return int(size[:-3]) * (2**20)\n",
    "    if size.upper().endswith(\"KIB\"):\n",
    "        return int(size[:-3]) * (2**10)\n",
    "    if size.upper().endswith(\"GB\"):\n",
    "        int_size = int(size[:-2]) * (10**9)\n",
    "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
    "    if size.upper().endswith(\"MB\"):\n",
    "        int_size = int(size[:-2]) * (10**6)\n",
    "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
    "    if size.upper().endswith(\"KB\"):\n",
    "        int_size = int(size[:-2]) * (10**3)\n",
    "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
    "    raise ValueError(\n",
    "        \"`size` is not in a valid format. Use an integer followed by the unit, e.g., '5GB'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WEIGHTS_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshard_checkpoint\u001b[39m(\n\u001b[1;32m      2\u001b[0m     state_dict: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m      3\u001b[0m     max_shard_size: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     weights_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m WEIGHTS_NAME,\n\u001b[1;32m      5\u001b[0m ):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Splits a model state dictionary in sub-checkpoints so that the final size of each sub-checkpoint does not exceed a\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    given size.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m            The name of the model save file.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     max_shard_size \u001b[38;5;241m=\u001b[39m convert_file_size_to_int(max_shard_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WEIGHTS_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "def shard_checkpoint(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    max_shard_size: Union[int, str] = \"10GB\",\n",
    "    weights_name: str = \"pytorch_model.bin\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a model state dictionary in sub-checkpoints so that the final size of each sub-checkpoint does not exceed a\n",
    "    given size.\n",
    "\n",
    "    The sub-checkpoints are determined by iterating through the `state_dict` in the order of its keys, so there is no\n",
    "    optimization made to make each sub-checkpoint as close as possible to the maximum size passed. For example, if the\n",
    "    limit is 10GB and we have weights of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get sharded as [6GB], [6+2GB],\n",
    "    [6+2+2GB] and not [6+2+2GB], [6+2GB], [6GB].\n",
    "\n",
    "    <Tip warning={true}>\n",
    "\n",
    "    If one of the model's weight is bigger than `max_shard_size`, it will end up in its own sub-checkpoint which will\n",
    "    have a size greater than `max_shard_size`.\n",
    "\n",
    "    </Tip>\n",
    "\n",
    "    Args:\n",
    "        state_dict (`Dict[str, torch.Tensor]`): The state dictionary of a model to save.\n",
    "        max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
    "            The maximum size of each sub-checkpoint. If expressed as a string, needs to be digits followed by a unit\n",
    "            (like `\"5MB\"`).\n",
    "        weights_name (`str`, *optional*, defaults to `\"pytorch_model.bin\"`):\n",
    "            The name of the model save file.\n",
    "    \"\"\"\n",
    "    max_shard_size = convert_file_size_to_int(max_shard_size)\n",
    "\n",
    "    sharded_state_dicts = [{}]\n",
    "    last_block_size = 0\n",
    "    total_size = 0\n",
    "    # storage_id_to_block = {}\n",
    "\n",
    "    for key, weight in state_dict.items():\n",
    "        # # when bnb serialization is used the weights in the state dict can be strings\n",
    "        # # check: https://github.com/huggingface/transformers/pull/24416 for more details\n",
    "        # if isinstance(weight, str):\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     storage_id = id_tensor_storage(weight)\n",
    "\n",
    "        # # If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`\n",
    "        # if storage_id in storage_id_to_block:\n",
    "        #     block_id = storage_id_to_block[storage_id]\n",
    "        #     sharded_state_dicts[block_id][key] = weight\n",
    "        #     continue\n",
    "\n",
    "        weight_size = weight.numel() * dtype_byte_size(weight.dtype)\n",
    "\n",
    "        # If this weight is going to tip up over the maximal size, we split, but only if we have put at least one\n",
    "        # weight in the current shard.\n",
    "        if (\n",
    "            last_block_size + weight_size > max_shard_size\n",
    "            and len(sharded_state_dicts[-1]) > 0\n",
    "        ):\n",
    "            sharded_state_dicts.append({})\n",
    "            last_block_size = 0\n",
    "\n",
    "        sharded_state_dicts[-1][key] = weight\n",
    "        last_block_size += weight_size\n",
    "        total_size += weight_size\n",
    "        # storage_id_to_block[storage_id] = len(sharded_state_dicts) - 1\n",
    "\n",
    "    # If we only have one shard, we return it\n",
    "    if len(sharded_state_dicts) == 1:\n",
    "        return {weights_name: sharded_state_dicts[0]}, None\n",
    "\n",
    "    # Otherwise, let's build the index\n",
    "    weight_map = {}\n",
    "    shards = {}\n",
    "    for idx, shard in enumerate(sharded_state_dicts):\n",
    "        shard_file = weights_name.replace(\n",
    "            \".bin\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.bin\"\n",
    "        )\n",
    "        # shard_file = shard_file.replace(\n",
    "        #     \".safetensors\",\n",
    "        #     f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.safetensors\",\n",
    "        # )\n",
    "        shards[shard_file] = shard\n",
    "        for key in shard.keys():\n",
    "            weight_map[key] = shard_file\n",
    "\n",
    "    # Add the metadata\n",
    "    metadata = {\"total_size\": total_size}\n",
    "    index = {\"metadata\": metadata, \"weight_map\": weight_map}\n",
    "    return shards, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balm.embed_tokens.weight: 42240 bytes\n",
      "balm.layers.0.norm1.weight: 1280 bytes\n",
      "balm.layers.0.norm1.bias: 1280 bytes\n",
      "balm.layers.0.norm2.weight: 1280 bytes\n",
      "balm.layers.0.norm2.bias: 1280 bytes\n",
      "balm.layers.0.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.0.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.0.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.0.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.0.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.0.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.0.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.0.feed_forward.2.bias: 1280 bytes\n",
      "balm.layers.1.norm1.weight: 1280 bytes\n",
      "balm.layers.1.norm1.bias: 1280 bytes\n",
      "balm.layers.1.norm2.weight: 1280 bytes\n",
      "balm.layers.1.norm2.bias: 1280 bytes\n",
      "balm.layers.1.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.1.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.1.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.1.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.1.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.1.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.1.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.1.feed_forward.2.bias: 1280 bytes\n",
      "balm.layers.2.norm1.weight: 1280 bytes\n",
      "balm.layers.2.norm1.bias: 1280 bytes\n",
      "balm.layers.2.norm2.weight: 1280 bytes\n",
      "balm.layers.2.norm2.bias: 1280 bytes\n",
      "balm.layers.2.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.2.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.2.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.2.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.2.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.2.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.2.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.2.feed_forward.2.bias: 1280 bytes\n",
      "balm.layers.3.norm1.weight: 1280 bytes\n",
      "balm.layers.3.norm1.bias: 1280 bytes\n",
      "balm.layers.3.norm2.weight: 1280 bytes\n",
      "balm.layers.3.norm2.bias: 1280 bytes\n",
      "balm.layers.3.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.3.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.3.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.3.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.3.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.3.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.3.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.3.feed_forward.2.bias: 1280 bytes\n",
      "balm.layers.4.norm1.weight: 1280 bytes\n",
      "balm.layers.4.norm1.bias: 1280 bytes\n",
      "balm.layers.4.norm2.weight: 1280 bytes\n",
      "balm.layers.4.norm2.bias: 1280 bytes\n",
      "balm.layers.4.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.4.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.4.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.4.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.4.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.4.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.4.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.4.feed_forward.2.bias: 1280 bytes\n",
      "balm.layers.5.norm1.weight: 1280 bytes\n",
      "balm.layers.5.norm1.bias: 1280 bytes\n",
      "balm.layers.5.norm2.weight: 1280 bytes\n",
      "balm.layers.5.norm2.bias: 1280 bytes\n",
      "balm.layers.5.attention.in_proj_weight: 1228800 bytes\n",
      "balm.layers.5.attention.in_proj_bias: 3840 bytes\n",
      "balm.layers.5.attention.out_proj.weight: 409600 bytes\n",
      "balm.layers.5.attention.out_proj.bias: 1280 bytes\n",
      "balm.layers.5.feed_forward.0.weight: 1638400 bytes\n",
      "balm.layers.5.feed_forward.0.bias: 5120 bytes\n",
      "balm.layers.5.feed_forward.2.weight: 819200 bytes\n",
      "balm.layers.5.feed_forward.2.bias: 1280 bytes\n",
      "balm.final_layer_norm.weight: 1280 bytes\n",
      "balm.final_layer_norm.bias: 1280 bytes\n",
      "lm_head.bias: 132 bytes\n",
      "lm_head.dense.weight: 409600 bytes\n",
      "lm_head.dense.bias: 1280 bytes\n",
      "lm_head.layer_norm.weight: 1280 bytes\n",
      "lm_head.layer_norm.bias: 1280 bytes\n",
      "lm_head.decoder.weight: 42240 bytes\n",
      "Total size: 25176452 bytes\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "\n",
    "for key, weight in balm.state_dict().items():\n",
    "    weight_size = weight.numel() * dtype_byte_size(weight.dtype)\n",
    "    print(f\"{key}: {weight_size} bytes\")\n",
    "    total_size += weight_size\n",
    "\n",
    "print(f\"Total size: {total_size} bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
