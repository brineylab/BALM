{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from balm.config import BaseConfig\n",
    "from balm.data import load_dataset, DataCollator\n",
    "from balm.embedding import RelativePositionalEmbedding\n",
    "from balm.modules import (\n",
    "    BalmClassificationHead,\n",
    "    BalmLMHead,\n",
    "    ClassifierOutput,\n",
    "    MaskedLMOutput,\n",
    "    RoformerLayer,\n",
    "    TransformerLayer,\n",
    "    DenseTransformerLayer,\n",
    ")\n",
    "from balm.models.base import BalmBase\n",
    "\n",
    "from balm.tokenizer import Tokenizer\n",
    "from balm.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalmConfig(BaseConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 320,\n",
    "        ffn_dim: int = 1280,\n",
    "        num_layers: int = 6,\n",
    "        num_heads: int = 20,\n",
    "        num_experts: int = 8,\n",
    "        max_length: int = 320,\n",
    "        vocab_size: int = 33,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.0,\n",
    "        token_embedding_dropout: float = 0.0,\n",
    "        positional_embedding_type: str = \"rotary\",\n",
    "        pre_norm: bool = True,\n",
    "        activation: str = \"swiglu\",\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        padding_idx: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Configuration for the Balm model. Default parameters are similar to the 8M parameter ESM-2 model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int, default=320\n",
    "            The dimension of the token embeddings.\n",
    "\n",
    "        ffn_dim : int, default=1280\n",
    "            The dimension of the feed-forward network.\n",
    "\n",
    "        num_layers : int, default=6\n",
    "            The number of layers in the transformer.\n",
    "\n",
    "        num_heads : int, default=20\n",
    "            The number of heads in the transformer.\n",
    "\n",
    "        num_experts : int, default=8\n",
    "            The number of experts in the transformer.\n",
    "\n",
    "        max_length : int, default=320\n",
    "            The maximum length of the input sequence.\n",
    "\n",
    "        vocab_size : int, default=33\n",
    "            The vocabulary size.\n",
    "\n",
    "        dropout : float, default=0.1\n",
    "            The dropout rate. Applied immediately before adding the residual connection.\n",
    "\n",
    "        attention_dropout : float, default=0.0\n",
    "            The dropout rate for the attention layer.\n",
    "\n",
    "        token_embedding_dropout : float, default=0.0\n",
    "            The dropout rate for the token embedding layer.\n",
    "\n",
    "        positional_embedding_type : str, default=\"rotary\"\n",
    "            The type of positional embedding to use. Options are \"rotary\" or \"relative\".\n",
    "\n",
    "        pre_norm : bool, default=True\n",
    "            Whether to use pre-norm or post-norm.\n",
    "\n",
    "        ffn_activation : str, default=\"swiglu\"\n",
    "            The activation function to use in the feed-forward network. Options are \"swiglu\", \"relu\", or \"gelu\".\n",
    "\n",
    "        layer_norm_eps : float, default=1e-5\n",
    "            The epsilon value for the layer normalization.\n",
    "\n",
    "        padding_idx : int, default=0\n",
    "            The index of the padding token.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.token_embedding_dropout = token_embedding_dropout\n",
    "        if positional_embedding_type.lower() not in [\"rotary\", \"relative\"]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid positional embedding type: {positional_embedding_type}. Options are 'rotary' or 'relative'.\"\n",
    "            )\n",
    "        self.positional_embedding_type = positional_embedding_type.lower()\n",
    "        if activation.lower() not in [\"swiglu\", \"relu\", \"gelu\" ]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid FFN activation: {activation}. Options are 'swiglu', 'relu', or 'gelu'.\"\n",
    "            )\n",
    "        self.activation = activation.lower()\n",
    "        self.pre_norm = pre_norm\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.padding_idx = padding_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalmModel(BalmBase):\n",
    "    config_cls = BalmConfig\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BalmConfig,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        BALM model, with rotary embeddings, pre-norm, and SwiGLU activations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : BalmConfig\n",
    "            The configuration object defining model architecture and hyperparameters.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # embedding\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            self.config.vocab_size,\n",
    "            self.config.embed_dim,\n",
    "            padding_idx=self.config.padding_idx,\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseTransformerLayer(\n",
    "                    self.config.embed_dim,\n",
    "                    self.config.ffn_dim,\n",
    "                    self.config.num_heads,\n",
    "                    self.config.max_length,\n",
    "                    dropout=self.config.dropout,\n",
    "                    attention_dropout=self.config.attention_dropout,\n",
    "                    token_embedding_dropout=self.config.token_embedding_dropout,\n",
    "                    layer_norm_eps=self.config.layer_norm_eps,\n",
    "                    activation=self.config.activation,\n",
    "                    positional_embedding_type=self.config.positional_embedding_type,\n",
    "                    pre_norm=self.config.pre_norm,\n",
    "                )\n",
    "                for _ in range(self.config.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.final_layer_norm = nn.LayerNorm(\n",
    "            self.config.embed_dim, eps=self.config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor. Expected shape is (batch_size, sequence_length).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor. The shape is (batch_size, sequence_length, embed_dim).\n",
    "        \"\"\"\n",
    "        x = self.embed_tokens(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights,\n",
    "            )\n",
    "            if need_weights:\n",
    "                x, attn = x\n",
    "        if self.config.pre_norm:\n",
    "            x = self.final_layer_norm(x)\n",
    "        if need_weights:\n",
    "            return x, attn\n",
    "        return x\n",
    "\n",
    "\n",
    "class BalmForMaskedLM(BalmBase):\n",
    "    config_cls = BalmConfig\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BalmConfig,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        BALM model for masked language modeling. Uses the base BALM model with rotary\n",
    "        embeddings, pre-norm, and SwiGLU activations, and adds a language modeling head.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : BalmConfig\n",
    "            The configuration object defining model architecture and hyperparameters.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.balm = BalmModel(config=self.config)\n",
    "        self.lm_head = BalmLMHead(self.config.embed_dim, self.config.vocab_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> MaskedLMOutput:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x : torch.Tensor\n",
    "            The input tensor. Expected shape is (batch_size, seq_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor. The shape is (batch_size, seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        x = self.balm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=output_attentions,\n",
    "        )\n",
    "        if output_attentions:\n",
    "            x, attn = x\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            masked_lm_loss = self.criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "\n",
    "        output = MaskedLMOutput(\n",
    "            logits=logits,\n",
    "            loss=masked_lm_loss,\n",
    "        )\n",
    "        if output_attentions:\n",
    "            output.attentions = attn\n",
    "        if output_hidden_states:\n",
    "            output.hidden_states = x\n",
    "        if return_dict:\n",
    "            return output.as_dict()\n",
    "        return output.as_tuple()\n",
    "\n",
    "\n",
    "class BalmForSequenceClassification(BalmBase):\n",
    "    config_cls = BalmConfig\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BalmConfig,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        BALM model for masked language modeling. Uses the base BALM model with rotary\n",
    "        embeddings, pre-norm, and SwiGLU activations, and adds a language modeling head.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : BalmConfig\n",
    "            The configuration object defining model architecture and hyperparameters.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # model\n",
    "        self.balm = BalmModel(config=self.config)\n",
    "\n",
    "        # classifier\n",
    "        classifier_dropout = (\n",
    "            self.config.classifier_dropout\n",
    "            if self.config.classifier_dropout is not None\n",
    "            else self.config.dropout\n",
    "        )\n",
    "        classifier_activation = (\n",
    "            self.config.classifier_activation\n",
    "            if self.config.classifier_activation is not None\n",
    "            else \"tanh\"\n",
    "        )\n",
    "        self.classifier = BalmClassificationHead(\n",
    "            embed_dim=self.config.embed_dim,\n",
    "            num_labels=self.config.num_labels,\n",
    "            dropout=classifier_dropout,\n",
    "            activation=classifier_activation,\n",
    "        )\n",
    "\n",
    "        # loss\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> MaskedLMOutput:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        x : torch.Tensor\n",
    "            The input tensor. Expected shape is (batch_size, seq_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor. The shape is (batch_size, seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        x = self.balm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=output_attentions,\n",
    "        )\n",
    "        if output_attentions:\n",
    "            x, attn = x\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        classifier_loss = None\n",
    "        if labels is not None:\n",
    "            classifier_loss = self.criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "\n",
    "        output = ClassifierOutput(\n",
    "            logits=logits,\n",
    "            loss=classifier_loss,\n",
    "        )\n",
    "        if output_attentions:\n",
    "            output.attentions = attn\n",
    "        if output_hidden_states:\n",
    "            output.hidden_states = x\n",
    "        if return_dict:\n",
    "            return output.as_dict()\n",
    "        return output.as_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "# data_files = {\n",
    "#     \"train\": \"../train-test-eval_paired/train.txt\",\n",
    "#     \"test\": \"../train-test-eval_paired/test.txt\",\n",
    "#     \"eval\": \"../train-test-eval_paired/eval.txt\",\n",
    "# }\n",
    "\n",
    "# data_files = {\n",
    "#     \"train\": \"../jaffe-plusHD_clust0.9_split/train.txt\",\n",
    "#     \"test\": \"../jaffe-plusHD_clust0.9_split/test.txt\",\n",
    "#     \"eval\": \"../jaffe-plusHD_clust0.9_split/eval.txt\",\n",
    "# }\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cbc6c3a79e467b914f71cafa194db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479134a6a5b430f83070f85a705e6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ff9a25a8284eb29e486d1c0c819303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    remove_columns=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched to ESM-2 8M\n",
    "config = BalmConfig(\n",
    "    embed_dim=320,\n",
    "    ffn_dim=320 * 4,\n",
    "    num_layers=6,\n",
    "    num_heads=20,\n",
    "    max_length=320,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "# model = BalmForMaskedLM(\n",
    "#     embed_dim=320,\n",
    "#     ffn_dim=320*4,\n",
    "#     num_layers=6,\n",
    "#     num_heads=20,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "# )\n",
    "model = BalmForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    output_dir=\"./training_runs/save_tests\",\n",
    "    epochs=1,\n",
    "    logging_steps=5,\n",
    "    eval_steps=100,\n",
    "    warmup_steps=10,\n",
    "    # save_steps=15,\n",
    "    per_device_train_batch_size=32,\n",
    "    # use_cpu=True,\n",
    "    # use_wandb=True,\n",
    "    wandb_project=\"test_wandb_logging\",\n",
    "    # wandb_entity=\"bryanbriney\",\n",
    "    run_name=\"save_test_001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1886105feb1544298723ce7c1d2e2b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/31 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5     | loss: 3.1664 | lr: 0.000200\n",
      "step 10    | loss: 2.7392 | lr: 0.000400\n",
      "step 15    | loss: 2.6835 | lr: 0.000305\n",
      "step 20    | loss: 2.6565 | lr: 0.000210\n",
      "step 25    | loss: 2.6667 | lr: 0.000114\n",
      "step 30    | loss: 2.6235 | lr: 0.000019\n",
      "<< SAVING FINAL MODEL >>\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
