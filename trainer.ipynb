{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass, field, fields\n",
    "from enum import Enum, StrEnum\n",
    "from typing import Optional, Tuple, List, Dict, Any, Iterable, Union\n",
    "\n",
    "from balm.data import load_dataset, DataCollator, Dataset\n",
    "from balm.models.balm import BalmForMaskedLM\n",
    "from balm.models.balm_moe import BalmMoEForMaskedLM\n",
    "from balm.models.balm_moe_rope import BalmMoERoPEForMaskedLM\n",
    "from balm.tokenizer import Tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcd6f0f4cb54dc9b1deae4c1168aff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d0fa80d4ed4165bc24fc39fb63926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9de6e4d1924f68bdf8743019f38c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    # remove_columns=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BalmMoERoPEForMaskedLM(\n",
    "model = BalmMoEForMaskedLM(\n",
    "    embed_dim=256,\n",
    "    ffn_dim=1024,\n",
    "    num_experts=4,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    expert_capacity=128,\n",
    "    router_z_loss_coef=0.01,\n",
    "    router_aux_loss_coef=0.01,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18982689"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer todos:\n",
    "* train/eval dataloader\n",
    "* move everything to the correct device (cuda or cpu)\n",
    "* learning rate (including scheduler)\n",
    "* gradient accumulation steps\n",
    "* logging\n",
    "* checkpointing\n",
    "* distributed training\n",
    "* gradient clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterationStrategy(Enum):\n",
    "    STEPS = \"steps\"\n",
    "    EPOCHS = \"epochs\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_strategy = IterationStrategy(\"steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_strategy == IterationStrategy.STEPS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitEnum(Enum):\n",
    "    def __init__(self, value):\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}.{self.name}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def _missing_(cls, value):\n",
    "        raise ValueError(\n",
    "            f\"{value} is not a valid {cls.__name__}, please select one of {list(cls._value2member_map_.keys())}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class IntervalStrategy(ExplicitEnum):\n",
    "    NO = \"no\"\n",
    "    STEPS = \"steps\"\n",
    "    EPOCHS = \"epochs\"\n",
    "\n",
    "\n",
    "class PaddingStrategy(ExplicitEnum):\n",
    "    LONGEST = \"longest\"\n",
    "    MAX_LENGTH = \"max_length\"\n",
    "    DO_NOT_PAD = \"do_not_pad\"\n",
    "\n",
    "\n",
    "class SchedulerType(ExplicitEnum):\n",
    "    LINEAR = \"linear\"\n",
    "    COSINE = \"cosine\"\n",
    "    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "    POLYNOMIAL = \"polynomial\"\n",
    "    CONSTANT = \"constant\"\n",
    "    CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "    INVERSE_SQRT = \"inverse_sqrt\"\n",
    "    REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n",
    "\n",
    "\n",
    "class OptimizerNames(ExplicitEnum):\n",
    "    \"\"\"\n",
    "    Stores the acceptable string identifiers for optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    ADAMW_HF = \"adamw_hf\"\n",
    "    ADAMW_TORCH = \"adamw_torch\"\n",
    "    ADAMW_TORCH_FUSED = \"adamw_torch_fused\"\n",
    "    ADAMW_TORCH_XLA = \"adamw_torch_xla\"\n",
    "    ADAMW_TORCH_NPU_FUSED = \"adamw_torch_npu_fused\"\n",
    "    ADAMW_APEX_FUSED = \"adamw_apex_fused\"\n",
    "    ADAFACTOR = \"adafactor\"\n",
    "    ADAMW_ANYPRECISION = \"adamw_anyprecision\"\n",
    "    SGD = \"sgd\"\n",
    "    ADAGRAD = \"adagrad\"\n",
    "    ADAMW_BNB = \"adamw_bnb_8bit\"\n",
    "    ADAMW_8BIT = \"adamw_8bit\"  # just an alias for adamw_bnb_8bit\n",
    "    LION_8BIT = \"lion_8bit\"\n",
    "    LION = \"lion_32bit\"\n",
    "    PAGED_ADAMW = \"paged_adamw_32bit\"\n",
    "    PAGED_ADAMW_8BIT = \"paged_adamw_8bit\"\n",
    "    PAGED_LION = \"paged_lion_32bit\"\n",
    "    PAGED_LION_8BIT = \"paged_lion_8bit\"\n",
    "    RMSPROP = \"rmsprop\"\n",
    "    RMSPROP_BNB = \"rmsprop_bnb\"\n",
    "    RMSPROP_8BIT = \"rmsprop_bnb_8bit\"\n",
    "    RMSPROP_32BIT = \"rmsprop_bnb_32bit\"\n",
    "    GALORE_ADAMW = \"galore_adamw\"\n",
    "    GALORE_ADAMW_8BIT = \"galore_adamw_8bit\"\n",
    "    GALORE_ADAFACTOR = \"galore_adafactor\"\n",
    "    GALORE_ADAMW_LAYERWISE = \"galore_adamw_layerwise\"\n",
    "    GALORE_ADAMW_8BIT_LAYERWISE = \"galore_adamw_8bit_layerwise\"\n",
    "    GALORE_ADAFACTOR_LAYERWISE = \"galore_adafactor_layerwise\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PaddingStrategy(\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaddingStrategy.MAX_LENGTH"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments for training a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : str\n",
    "        The output directory where the model predictions and checkpoints will be written.\n",
    "\n",
    "    overwrite_output_dir : bool, default=False\n",
    "        Overwrite the content of the output directory. Use this to continue training\n",
    "        if output_dir points to a checkpoint directory.\n",
    "\n",
    "    evaluation_strategy : Union[IntervalStrategy, str], default=\"no\"\n",
    "        The evaluation strategy to use. Possible values are:\n",
    "\n",
    "            - `\"no\"`: No evaluation is done during training.\n",
    "            - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
    "            - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
    "\n",
    "    per_device_train_batch_size : int, default=8\n",
    "        Batch size per GPU/MPS core or CPU for training.\n",
    "\n",
    "    per_device_eval_batch_size : int, default=8\n",
    "        Batch size per GPU/MPS core or CPU for evaluation.\n",
    "\n",
    "    gradient_accumulation_steps : int, default=1\n",
    "        Number of updates steps to accumulate before performing a backward/update pass.\n",
    "\n",
    "        ..note::\n",
    "            When using gradient accumulation, a step is only counted for logging, evaluation,\n",
    "            and saving if it inclucdes a backward pass. Therefore, logging, evaluation, and\n",
    "            save operations will be conducted every `gradient_accumulation_steps * xxx_step`\n",
    "            training examples.\n",
    "\n",
    "    learning_rate : float, default=5e-5\n",
    "        The initial learning rate for AdamW.\n",
    "\n",
    "    adam_beta1 : float, default=0.9\n",
    "        Beta1 for AdamW optimizer.\n",
    "\n",
    "    adam_beta2 : float, default=0.999\n",
    "        Beta2 for AdamW optimizer.\n",
    "\n",
    "    adam_epsilon : float, default=1e-8\n",
    "        Epsilon for AdamW optimizer.\n",
    "\n",
    "    max_grad_norm : float, default=1.0\n",
    "        Max gradient norm  (for gradient clipping).\n",
    "\n",
    "    max_epochs : int, default=3\n",
    "        Total number of training epochs to perform.\n",
    "\n",
    "    max_steps : int, default=-1\n",
    "        If set to a positive number, the total number of training steps to perform.\n",
    "        Overrides `num_train_epochs`. For a finite dataset, training is reiterated\n",
    "        through the dataset (if all data is exhausted) until `max_steps` is reached.\n",
    "\n",
    "    lr_scheduler_type : Union[SchedulerType, str], default=\"linear\"\n",
    "        The scheduler type to use.\n",
    "\n",
    "    lr_scheduler_kwargs : Dict, default={}\n",
    "        Extra parameters for the lr_scheduler such as {'num_cycles': 1} for the\n",
    "        cosine with hard restarts.\n",
    "\n",
    "    warmup_ratio : float, default=0.0\n",
    "        Linear warmup over warmup_ratio fraction of total steps.\n",
    "\n",
    "    warmup_steps : int, default=0\n",
    "        Linear warmup over warmup_steps.\n",
    "\n",
    "    logging_dir : str, default=None\n",
    "        Tensorboard log dir.\n",
    "\n",
    "    logging_strategy : Union[IntervalStrategy, str], default=\"steps\"\n",
    "        The logging strategy to use.\n",
    "\n",
    "    logging_first_step : bool, default=False\n",
    "        Whether to log the first global_step.\n",
    "\n",
    "    logging_steps : float, default=500\n",
    "        Log every X updates steps. Should be an integer or a float in range `[0,1)`.\n",
    "        If smaller than 1, will be interpreted as ratio of total training steps.\n",
    "\n",
    "    save_strategy : Union[IntervalStrategy, str], default=\"steps\"\n",
    "        The saving strategy to use.\n",
    "\n",
    "    save_steps : float, default=500\n",
    "        Save every X updates steps. Should be an integer or a float in range `[0,1)`.\n",
    "        If smaller than 1, will be interpreted as ratio of total training steps.\n",
    "\n",
    "    save_total_limit : int, default=None\n",
    "        Maximum number of checkpoints to save.\n",
    "\n",
    "    use_cpu : bool, default=False\n",
    "        Whether to use CPU instead of GPU.\n",
    "\n",
    "    seed : int, default=42\n",
    "        Random seed that will be set for reproducibility.\n",
    "\n",
    "    data_seed : int, default=None\n",
    "        Random seed to be used with data samplers. If not set, random generators\n",
    "        for data sampling will use the same seed as `seed`. This can be used to\n",
    "        ensure reproducibility of data sampling, independent of the model seed.\n",
    "\n",
    "    fp16 : bool, default=False\n",
    "        Whether to use 16-bit precision instead of 32-bit.\n",
    "\n",
    "    ddp_backend : str, default=\"nccl\"\n",
    "        Backend to use for distributed training.\n",
    "\n",
    "    dataloader_drop_last : bool, default=False\n",
    "        Whether to drop the last incomplete batch.\n",
    "\n",
    "    eval_steps : int, default=None\n",
    "        Number of steps to run the evaluation for.\n",
    "\n",
    "    run_name : str, default=None\n",
    "        Name of the run.\n",
    "\n",
    "    disable_tqdm : bool, default=False\n",
    "        Whether to disable the tqdm progress bar.\n",
    "\n",
    "    remove_unused_columns : bool, default=False\n",
    "        Whether to remove unused columns from the dataset.\n",
    "\n",
    "    label_names : List[str], default=None\n",
    "        Names of the labels.\n",
    "\n",
    "    accelerator_config : dict, default=None\n",
    "        Configuration for the accelerator.\n",
    "\n",
    "    deepspeed : str, default=None\n",
    "        Configuration for DeepSpeed.\n",
    "\n",
    "    label_smoothing_factor : float, default=0.0\n",
    "        The factor by which to smooth the labels.\n",
    "\n",
    "    optim : dict, default=None\n",
    "        The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused,\n",
    "        adamw_apex_fused, adamw_anyprecision, or adafactor.\n",
    "\n",
    "    optim_args : dict, default=None\n",
    "        Arguments for the optimizer.\n",
    "\n",
    "    report_to : str, default=None\n",
    "        The platform to report the results to: wandb, tensorboard, or none.\n",
    "\n",
    "    dataloader_pin_memory : bool, default=True\n",
    "        Whether to pin memory for the dataloaders.\n",
    "\n",
    "    dataloader_num_workers : int, default=0\n",
    "        The number of workers to use for the dataloaders.\n",
    "\n",
    "    resume_from_checkpoint : str, default=None\n",
    "        The path to the checkpoint from which to resume training.\n",
    "\n",
    "    include_inputs_for_metrics : bool, default=False\n",
    "        Whether or not the inputs will be passed to the `compute_metrics` function. \n",
    "        This is intended for metrics that need inputs, predictions and references \n",
    "        for scoring calculation in Metric class.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    output_dir: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"The output directory where the model predictions and checkpoints will be written.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_output_dir: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Overwrite the content of the output directory. \"\n",
    "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    evaluation_strategy: Union[IntervalStrategy, str] = field(\n",
    "        default=\"no\",\n",
    "        metadata={\"help\": \"The evaluation strategy to use.\"},\n",
    "    )\n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n",
    "    )\n",
    "    per_device_eval_batch_size: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n",
    "    )\n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n",
    "        },\n",
    "    )\n",
    "    eval_delay: Optional[float] = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\"\n",
    "                \" evaluation_strategy.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    learning_rate: float = field(\n",
    "        default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n",
    "    )\n",
    "    weight_decay: float = field(\n",
    "        default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"}\n",
    "    )\n",
    "    adam_beta1: float = field(\n",
    "        default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"}\n",
    "    )\n",
    "    adam_beta2: float = field(\n",
    "        default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"}\n",
    "    )\n",
    "    adam_epsilon: float = field(\n",
    "        default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"}\n",
    "    )\n",
    "    max_grad_norm: float = field(default=1.0, metadata={\"help\": \"Max gradient norm.\"})\n",
    "\n",
    "    num_train_epochs: float = field(\n",
    "        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n",
    "    )\n",
    "    max_steps: int = field(\n",
    "        default=-1,\n",
    "        metadata={\n",
    "            \"help\": \"If > 0: set total number of training steps to perform. Override num_train_epochs.\"\n",
    "        },\n",
    "    )\n",
    "    lr_scheduler_type: Union[SchedulerType, str] = field(\n",
    "        default=\"linear\",\n",
    "        metadata={\"help\": \"The scheduler type to use.\"},\n",
    "    )\n",
    "    lr_scheduler_kwargs: Optional[Dict] = field(\n",
    "        default_factory=dict,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Extra parameters for the lr_scheduler such as {'num_cycles': 1} for the cosine with hard restarts\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    warmup_ratio: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Linear warmup over warmup_ratio fraction of total steps.\"},\n",
    "    )\n",
    "    warmup_steps: int = field(\n",
    "        default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"}\n",
    "    )\n",
    "    logging_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Tensorboard log dir.\"}\n",
    "    )\n",
    "    logging_strategy: Union[IntervalStrategy, str] = field(\n",
    "        default=\"steps\",\n",
    "        metadata={\"help\": \"The logging strategy to use.\"},\n",
    "    )\n",
    "    logging_first_step: bool = field(\n",
    "        default=False, metadata={\"help\": \"Log the first global_step\"}\n",
    "    )\n",
    "    logging_steps: float = field(\n",
    "        default=500,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n",
    "                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    save_strategy: Union[IntervalStrategy, str] = field(\n",
    "        default=\"steps\",\n",
    "        metadata={\"help\": \"The checkpoint save strategy to use.\"},\n",
    "    )\n",
    "    save_steps: float = field(\n",
    "        default=500,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n",
    "                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    save_total_limit: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\"\n",
    "                \" `output_dir`. When `load_best_model_at_end` is enabled, the 'best' checkpoint according to\"\n",
    "                \" `metric_for_best_model` will always be retained in addition to the most recent ones. For example,\"\n",
    "                \" for `save_total_limit=5` and `load_best_model_at_end=True`, the four last checkpoints will always be\"\n",
    "                \" retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end=True`,\"\n",
    "                \" it is possible that two checkpoints are saved: the last one and the best one (if they are different).\"\n",
    "                \" Default is unlimited checkpoints\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_cpu: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \" Whether or not to use cpu. If set to False, we will use cuda/tpu/mps/npu device if available.\"\n",
    "        },\n",
    "    )\n",
    "    seed: int = field(\n",
    "        default=42,\n",
    "        metadata={\"help\": \"Random seed that will be set at the beginning of training.\"},\n",
    "    )\n",
    "    data_seed: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"Random seed to be used with data samplers.\"}\n",
    "    )\n",
    "    fp16: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use fp16 (mixed) precision instead of 32-bit\"},\n",
    "    )\n",
    "    ddp_backend: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The backend to be used for distributed training\",\n",
    "            \"choices\": [\"nccl\", \"gloo\", \"mpi\", \"ccl\", \"hccl\"],\n",
    "        },\n",
    "    )\n",
    "    dataloader_drop_last: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Drop the last incomplete batch if it is not divisible by the batch size.\"\n",
    "        },\n",
    "    )\n",
    "    eval_steps: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n",
    "                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    run_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional descriptor for the run. Notably used for wandb logging.\"\n",
    "        },\n",
    "    )\n",
    "    disable_tqdm: Optional[bool] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Whether or not to disable the tqdm progress bars.\"},\n",
    "    )\n",
    "\n",
    "    remove_unused_columns: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Remove columns not required by the model when using an nlp.Dataset.\"\n",
    "        },\n",
    "    )\n",
    "    label_names: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The list of keys in your dictionary of inputs that correspond to the labels.\"\n",
    "        },\n",
    "    )\n",
    "    # Do not touch this type annotation or it will stop working in CLI\n",
    "    accelerator_config: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Config to be used with the internal Accelerator object initializtion. The value is either a \"\n",
    "                \"accelerator json config file (e.g., `accelerator_config.json`) or an already loaded json file as `dict`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    # Do not touch this type annotation or it will stop working in CLI\n",
    "    deepspeed: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Enable deepspeed and pass the path to deepspeed json config file (e.g. `ds_config.json`) or an already\"\n",
    "                \" loaded json file as a dict\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    label_smoothing_factor: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    default_optim = \"adamw_torch\"\n",
    "    # XXX: enable when pytorch==2.0.1 comes out - we want to give it time to get all the bugs sorted out\n",
    "    # if is_torch_available() and version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"2.1.0\"):\n",
    "    #     default_optim = \"adamw_torch_fused\"\n",
    "    # and update the doc above to:\n",
    "    # optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch_fused\"` (for torch<2.1.0 `\"adamw_torch\"`):\n",
    "    optim: Union[OptimizerNames, str] = field(\n",
    "        default=default_optim,\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    optim_args: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Optional arguments to supply to optimizer.\"}\n",
    "    )\n",
    "    report_to: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The list of integrations to report the results and logs to.\"\n",
    "        },\n",
    "    )\n",
    "    dataloader_pin_memory: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether or not to pin memory for DataLoader.\"}\n",
    "    )\n",
    "    dataloader_persistent_workers: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"If True, the data loader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will increase RAM usage.\"\n",
    "        },\n",
    "    )\n",
    "    resume_from_checkpoint: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to a folder with a valid checkpoint for your model.\"\n",
    "        },\n",
    "    )\n",
    "    include_inputs_for_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether or not the inputs will be passed to the `compute_metrics` function.\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        data_collator: DataCollator, \n",
    "        optimizer: Optional[Optimizer] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.data_collator = data_collator\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        self._device = None\n",
    "        self._train_dataloader = None\n",
    "        self._eval_dataloader = None\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        The device to run the model on. \n",
    "        Will check for CUDA, MPS (Apple Silicon), and CPU in that order.\n",
    "        \"\"\"\n",
    "        if self._device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self._device = torch.device(\"cuda\")\n",
    "            elif torch.backends.mps.is_available():\n",
    "                self._device = torch.device(\"mps\")\n",
    "            else:\n",
    "                self._device = torch.device(\"cpu\")\n",
    "        return self._device\n",
    "    \n",
    "    @property\n",
    "    def train_dataloader(self):\n",
    "        if self._train_dataloader is None:\n",
    "            self._train_dataloader = DataLoader(\n",
    "                self.train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        return self._train_dataloader\n",
    "    \n",
    "    @property\n",
    "    def eval_dataloader(self):\n",
    "        if self._eval_dataloader is None:\n",
    "            self._eval_dataloader = DataLoader(\n",
    "                self.eval_dataset,\n",
    "                batch_size=self.eval_batch_size,\n",
    "                shuffle=False,\n",
    "            )\n",
    "        return self._eval_dataloader\n",
    "        \n",
    "\n",
    "    def train(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = self.collator(batch)\n",
    "            outputs = self.model(**inputs)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, collator):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.collator = collator\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = self.collator(batch)\n",
    "            outputs = self.model(**inputs)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b5fd91410943cc966dc376bf6bfd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, total Loss: 0.3494, LM loss: 0.3329, router z loss: 0.0066, router aux loss: 0.0099  \n",
      "step: 10, total Loss: 0.3194, LM loss: 0.3092, router z loss: 0.0005, router aux loss: 0.0098  \n",
      "step: 15, total Loss: 0.3203, LM loss: 0.3099, router z loss: 0.0004, router aux loss: 0.0100  \n",
      "step: 20, total Loss: 0.3068, LM loss: 0.2969, router z loss: 0.0003, router aux loss: 0.0096  \n",
      "step: 25, total Loss: 0.2884, LM loss: 0.2781, router z loss: 0.0007, router aux loss: 0.0096  \n",
      "step: 30, total Loss: 0.3100, LM loss: 0.2995, router z loss: 0.0008, router aux loss: 0.0097  \n",
      "step: 35, total Loss: 0.2651, LM loss: 0.2547, router z loss: 0.0006, router aux loss: 0.0098  \n",
      "step: 40, total Loss: 0.2867, LM loss: 0.2759, router z loss: 0.0007, router aux loss: 0.0100  \n",
      "step: 45, total Loss: 0.2733, LM loss: 0.2627, router z loss: 0.0007, router aux loss: 0.0099  \n",
      "step: 50, total Loss: 0.2584, LM loss: 0.2482, router z loss: 0.0005, router aux loss: 0.0097  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# outputs = model(\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     input_ids=collated[\"input_ids\"],\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     labels=collated.get(\"labels\", None),\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     key_padding_mask=collated.get(\"attention_mask\", None),\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# pbar.update(train_dataloader.batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "model.train()\n",
    "# pbar = tqdm(total=len(train_dataloader) * train_dataloader.batch_size * n_epochs)\n",
    "pbar = tqdm(total=len(train_dataloader) * n_epochs)\n",
    "n_steps = 0\n",
    "pbar.reset()\n",
    "for epoch in range(n_epochs):\n",
    "    for examples in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        collated = collator(examples[\"input_ids\"])\n",
    "\n",
    "        input_ids = collated[\"input_ids\"].to(device)\n",
    "        labels = collated.get(\"labels\", None)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "        attention_mask = collated.get(\"attention_mask\", None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, key_padding_mask=attention_mask)\n",
    "\n",
    "        # outputs = model(\n",
    "        #     input_ids=collated[\"input_ids\"],\n",
    "        #     labels=collated.get(\"labels\", None),\n",
    "        #     key_padding_mask=collated.get(\"attention_mask\", None),\n",
    "        # )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # pbar.update(train_dataloader.batch_size)\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "        n_steps += 1\n",
    "        if n_steps % 5 == 0:\n",
    "            print(\n",
    "                f\"step: {n_steps}, total Loss: {loss.item():.4f}, LM loss: {outputs.lm_loss.item():.4f}, router z loss: {outputs.router_z_loss.item():.4f}, router aux loss: {outputs.router_aux_loss.item():.4f}  \"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
