{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass, field, fields\n",
    "from enum import Enum, StrEnum\n",
    "from typing import Optional, Tuple, List, Dict, Any, Iterable, Union\n",
    "\n",
    "from balm.data import load_dataset, DataCollator, Dataset\n",
    "from balm.models.balm import BalmForMaskedLM\n",
    "from balm.models.balm_moe import BalmMoEForMaskedLM\n",
    "from balm.models.balm_moe_rope import BalmMoERoPEForMaskedLM\n",
    "from balm.tokenizer import Tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e855309cfc46491ebdf5b9a50f038e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9697e2d993d9430cb89ccb473124991a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056cc3795b304b4c93fd1d6870fa7421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    # remove_columns=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BalmMoERoPEForMaskedLM(\n",
    "model = BalmMoEForMaskedLM(\n",
    "    embed_dim=256,\n",
    "    ffn_dim=1024,\n",
    "    num_experts=4,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    expert_capacity=128,\n",
    "    router_z_loss_coef=0.01,\n",
    "    router_aux_loss_coef=0.01,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "# model = model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=4e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18982689"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer todos:\n",
    "* train/eval dataloader\n",
    "* move everything to the correct device (cuda or cpu)\n",
    "* learning rate (including scheduler)\n",
    "* gradient accumulation steps\n",
    "* logging\n",
    "* checkpointing\n",
    "* distributed training\n",
    "* gradient clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExplicitEnum(Enum):\n",
    "#     def __init__(self, value):\n",
    "#         self._value = value\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f\"{self.__class__.__name__}.{self.name}\"\n",
    "    \n",
    "#     @classmethod\n",
    "#     def _missing_(cls, value):\n",
    "#         raise ValueError(\n",
    "#             f\"{value} is not a valid {cls.__name__}, please select one of {list(cls._value2member_map_.keys())}\"\n",
    "#         )\n",
    "\n",
    "\n",
    "# class IntervalStrategy(ExplicitEnum):\n",
    "#     NO = \"no\"\n",
    "#     STEPS = \"steps\"\n",
    "#     EPOCHS = \"epochs\"\n",
    "\n",
    "\n",
    "# class PaddingStrategy(ExplicitEnum):\n",
    "#     LONGEST = \"longest\"\n",
    "#     MAX_LENGTH = \"max_length\"\n",
    "#     DO_NOT_PAD = \"do_not_pad\"\n",
    "\n",
    "\n",
    "# class SchedulerType(ExplicitEnum):\n",
    "#     LINEAR = \"linear\"\n",
    "#     COSINE = \"cosine\"\n",
    "#     COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "#     POLYNOMIAL = \"polynomial\"\n",
    "#     CONSTANT = \"constant\"\n",
    "#     CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "#     INVERSE_SQRT = \"inverse_sqrt\"\n",
    "#     REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n",
    "\n",
    "\n",
    "# class OptimizerNames(ExplicitEnum):\n",
    "#     \"\"\"\n",
    "#     Stores the acceptable string identifiers for optimizers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     ADAMW_HF = \"adamw_hf\"\n",
    "#     ADAMW_TORCH = \"adamw_torch\"\n",
    "#     ADAMW_TORCH_FUSED = \"adamw_torch_fused\"\n",
    "#     ADAMW_TORCH_XLA = \"adamw_torch_xla\"\n",
    "#     ADAMW_TORCH_NPU_FUSED = \"adamw_torch_npu_fused\"\n",
    "#     ADAMW_APEX_FUSED = \"adamw_apex_fused\"\n",
    "#     ADAFACTOR = \"adafactor\"\n",
    "#     ADAMW_ANYPRECISION = \"adamw_anyprecision\"\n",
    "#     SGD = \"sgd\"\n",
    "#     ADAGRAD = \"adagrad\"\n",
    "#     ADAMW_BNB = \"adamw_bnb_8bit\"\n",
    "#     ADAMW_8BIT = \"adamw_8bit\"  # just an alias for adamw_bnb_8bit\n",
    "#     LION_8BIT = \"lion_8bit\"\n",
    "#     LION = \"lion_32bit\"\n",
    "#     PAGED_ADAMW = \"paged_adamw_32bit\"\n",
    "#     PAGED_ADAMW_8BIT = \"paged_adamw_8bit\"\n",
    "#     PAGED_LION = \"paged_lion_32bit\"\n",
    "#     PAGED_LION_8BIT = \"paged_lion_8bit\"\n",
    "#     RMSPROP = \"rmsprop\"\n",
    "#     RMSPROP_BNB = \"rmsprop_bnb\"\n",
    "#     RMSPROP_8BIT = \"rmsprop_bnb_8bit\"\n",
    "#     RMSPROP_32BIT = \"rmsprop_bnb_32bit\"\n",
    "#     GALORE_ADAMW = \"galore_adamw\"\n",
    "#     GALORE_ADAMW_8BIT = \"galore_adamw_8bit\"\n",
    "#     GALORE_ADAFACTOR = \"galore_adafactor\"\n",
    "#     GALORE_ADAMW_LAYERWISE = \"galore_adamw_layerwise\"\n",
    "#     GALORE_ADAMW_8BIT_LAYERWISE = \"galore_adamw_8bit_layerwise\"\n",
    "#     GALORE_ADAFACTOR_LAYERWISE = \"galore_adafactor_layerwise\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from balm.training.training_arguments import TrainingArguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         model: nn.Module, \n",
    "#         data_collator: DataCollator, \n",
    "#         optimizer: Optional[Optimizer] = None,\n",
    "#         train_dataset: Optional[Dataset] = None,\n",
    "#         eval_dataset: Optional[Dataset] = None,\n",
    "#         batch_size: int = 32,\n",
    "#         eval_batch_size: int = 32,\n",
    "#     ):\n",
    "#         self.model = model\n",
    "#         self.data_collator = data_collator\n",
    "#         self.optimizer = optimizer\n",
    "#         self.batch_size = batch_size\n",
    "#         self.eval_batch_size = eval_batch_size\n",
    "#         self.train_dataset = train_dataset\n",
    "#         self.eval_dataset = eval_dataset\n",
    "\n",
    "#         self._device = None\n",
    "#         self._train_dataloader = None\n",
    "#         self._eval_dataloader = None\n",
    "\n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         \"\"\"\n",
    "#         The device to run the model on. \n",
    "#         Will check for CUDA, MPS (Apple Silicon), and CPU in that order.\n",
    "#         \"\"\"\n",
    "#         if self._device is None:\n",
    "#             if torch.cuda.is_available():\n",
    "#                 self._device = torch.device(\"cuda\")\n",
    "#             elif torch.backends.mps.is_available():\n",
    "#                 self._device = torch.device(\"mps\")\n",
    "#             else:\n",
    "#                 self._device = torch.device(\"cpu\")\n",
    "#         return self._device\n",
    "    \n",
    "#     @property\n",
    "#     def train_dataloader(self):\n",
    "#         if self._train_dataloader is None:\n",
    "#             self._train_dataloader = DataLoader(\n",
    "#                 self.train_dataset,\n",
    "#                 batch_size=self.batch_size,\n",
    "#                 shuffle=True,\n",
    "#             )\n",
    "#         return self._train_dataloader\n",
    "    \n",
    "#     @property\n",
    "#     def eval_dataloader(self):\n",
    "#         if self._eval_dataloader is None:\n",
    "#             self._eval_dataloader = DataLoader(\n",
    "#                 self.eval_dataset,\n",
    "#                 batch_size=self.eval_batch_size,\n",
    "#                 shuffle=False,\n",
    "#             )\n",
    "#         return self._eval_dataloader\n",
    "        \n",
    "\n",
    "#     def train(self, dataloader):\n",
    "#         self.model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch in tqdm(dataloader):\n",
    "#             inputs = self.collator(batch)\n",
    "#             outputs = self.model(**inputs)\n",
    "#             loss = outputs[\"loss\"]\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "#             self.optimizer.zero_grad()\n",
    "#             total_loss += loss.item()\n",
    "#         return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from balm.training.trainer import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    data_collator=collator, \n",
    "    train_dataset=tokenized_dataset[\"train\"], \n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=32,\n",
    "    # per_device_eval_batch_size=32,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.n_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "for dl in trainer.train_dataloader:\n",
    "    print(dl.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b1033a5a4744549b30c4e93d512048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['EVQLVESGGVVVQPGGSLRLSCAASGFTFDDYAMHWVRQAPGKGLEWVSLISWDGGSTYYADSVKGRFTISRDNSKNSLYLQMNSLRAEDTALYYCAKDISGLTHPGYYDSSGYYSLGSWGQGTLVTVSS<cls><cls>QSALTQPASVSGSPGQSITISCTGTSSDVGGYNYVSWYQQHPGKAPKLMIYDVSNRPSGVSNRFSGSKSGNTASLTISGLQAEDEADYYCSSYTSSSTLAFGGGTKLTVL', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGWINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCARDLQGNQYSSGWSWGQGTLVTVSS<cls><cls>QSVLTQPPSVSAAPGQKVTISCSGSSSNIGNNYVSWYQQLPGTAPKLLIYDNNKRPSGIPDRFSGSKSGTSATLGITGLQTGDEADYYCGTWDSSLSAVVFGGGTKLTVL', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGWINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCARVRQYCSSTSCYLPDAFDIWGQGTMVTVSS<cls><cls>DIQMTQSPSTLSASVGDRVTITCRASQSISSWLAWYQQKPGKAPKLLIYKASSLESGVPSRFSGSGSGTEFTLTISSLQPDDFATYYCQQYNSYSRTFGQGTKVEIK', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKGDSMGFKPALFDYWGQGTLVTVSS<cls><cls>SYVLTQPPSVSVAPGQTARITCGGNNIGSKSVHWYQQKPGQAPVLVVYDDSDRPSGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDSSSDHPVFGGGTKLTVL', 'EVQLVESGGGLIQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGLEWVSVIYSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCARDSVVPAAISYYYYGMDVWGQGTTVTVSS<cls><cls>SYELTQPPSVSVSPGQTARITCSGDALPKQYAYWYQQKPGQAPVLVIYKDSERPSGIPERFSGSSSGTTVTLTISGVQAEDEADYYCQSADSSGTYPGVFGGGTKLTVL', 'EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYSMNWVRQAPGKGLEWVSYISSSSSTIYYADSVKGRFTISRDNAKNSLYLQMNSLRDEDTAVYYCARSGDILTGYYPPFDYWGQGTLVTVSS<cls><cls>SYELTQPPSVSVSPGQTASITCSGDKLGDKYACWYQQKPGQSPVLVIYQDSKRPSGIPERFSGSNSGNTATLTISGTQAMDEADYYCQAWDPRGYVFGTGTKVTVL', 'EVQLVESGGGLVQPGGSLRLSCAASGFTFRRYWMTWVRQAPGKGLEWEANINPDGSAKYYMDSVRGRFTISRDNAKNSLYLQMNSLRAEDTAVYYCARIGYSSSSLDFWGQGTVVTVSS<cls><cls>QSVLTQPPSVSAAPGQRVTISCSGSSSNIGNNNVSWYQHLPGTAPKLLIFDNNKRPSGIPDRFSGSKSGTSATLGITGLQTGDGADYYCGTWDTSLTALVFGGGTKLTV', 'EVQLVESGGGLVQPGGSLRLSCGASGFTFSSYWMSWVRQAPGKGLEWVANIKQDGSEKYYADSVKGRFTISRDNAKNSLYLQVNSLRAEDTAVYYCARGSTESIYRYFDYWGQGTLVTVSS<cls><cls>DIVMTQSPDSLAVSLGERATINCKSSQSVLSSSNNKNYLGWYQQKSGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTINSLQTEDVAVYYCQQYYSKQYTFGQGTKLEIK', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKELPGAYGDYYYYYGMDVWGQGTTVTVSS<cls><cls>SSELTQDPAVSVALGQTVRITCQGDSLRSYYASWYQQKPGQAPVLVIYGKNNRPSGIPDRFSGSSSGNTASLTITGAQAEDEADYYCNSRDSSGNPWVFGGGTKLTVL', 'QITLKESGPTLVKPTQTLTLTCTFSGFSLSTSGVGVGWIRQPPGKALEWLAVIYANNDRRYSASLKSRLLITKDTSKNQVVLTMTNMDPVDTGTYFCAHRLQPGIWFDPWGQGVLVTVSS<cls><cls>QSALTQPPSASGSPGQSVTISCTGTSSDVGAYNYVSWYQQHPGKAPKLIIYLVTKRPSGVPDRFSGSKSGNTASLTVSGLQADDEADYYCCSHGGSTNYMIFGGGTKLTVL', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGRINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCAREGGRKDYYYGMDVWGQGTTVTVSS<cls><cls>DIQMTQSPSSLSASVGDRVTITCQASQDISNYLNWYQQKPGKAPKLLIYDASNLETGVPSRFSGSGSGTDFTFTISSLQPEDIATYYCQQYDNLPLTFGGGTKVEIK', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQRLEWMGWINAGNGNTKYSQKFQGRVTITRDTSASTAYMELSSLRSEDTAVYYCATAVADGVAYWGQGTLVTVSS<cls><cls>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPEDFAVYYCQQYGSSPWTFGQGTKVEIK', 'EVQLLESGGGLVQPGGSLRLSCGASGFIFSTYAMSWVRQAPGKGLEWVSTISGSGASTYYADSVKGRFTISRDNSKNTVFLQINSLRVEDTAVYYCAKDGGSRIQLWYRQYWGQGTLVTVSS<cls><cls>QSVLTQPPSVSEAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPKLLMYGNSNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSYDSSLSGWVFGGGTKLTVL', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKAVRFLEWLPLDGMDVWGQGTTVTVSS<cls><cls>QSVLTQPPSASGTPGQRVTISCSGSSSNIGSNTVNWYQQLPGTAPKLLIYSNNQRPSGVPDRFSGSKSGTSASLAISGLQSEDEADYYCAAWDDSLNALYVFGTGTKVTVL', 'EVLVVESGGGLVQPGGSLRLSCAASGFFSSDYSMNWVRRAPGKGLEWLSYLGNDGRTIYYADSVKGRFATSRDSAKNSMYLQMNSLRDDDTAVYYCARDGGRAYEMDVWGQGTTVTVSS<cls><cls>QSALTQPASVSGSPGQSITISCTGTISDIGTYNYLCWYQHHPGKAPKLIIYGVSNRPSGVSNRFSGSKSGNTASLTISGLQAEDEADYYCMSYRRSSTLVFGGGTRLTVV', 'EVQLVESGGDLVQPGGSLRLSCAASGFTFSEYWMHWVRRAPGKGLEWIFSLNGDGTTIIHADSVKGRFTVSRDNAKNTLFLQMNSLRAEDTAVYYCTREDCSGGFCKKFEYWGQGTLVTVSS<cls><cls>VMTQTPSSSPVTVGQPASISCTSSQSLVHSDGNTYLSWLQQRPGQPPRVLIYALSNRFSGVPDRFSGSGAGTDFTLKISRVEAEDVGVYYCMQGTHFPYTFGQGTKLEIK', 'QVTLRESGPALVKPTQTLTLTCTFSGFSLSTSGMCVSWVRQPPGKALEWLALIDWDDDKYYSTSLKTRLTISKDTSKNQVVLTMTNMDPVDTATYYCARMDYGDYWGQGTLVTVSS<cls><cls>DIQMTQSPSTLSASVGDRVTITCRASQSISSWLAWYQQKPGKAPKLLIYKASSLESGVPSRFSGSGSGTEFTLTISSLQPDDFATYYCQQYNSYPFTFGPGTKVDIK', 'EVQLVESGGGLVQPGGSLRLSCAASGFTFSNYWMSWVRQAPGKGLEWVANIKEDGSKSYYVDSVKGRFTISRDNAKNSLYLQMNSLRAEDTAVYYCARGIVVLSYSGFAFDIWGQGTTVTVSS<cls><cls>EIVLTQSPDFQSVTPKEKVTITCRASQSIGSSLHWYQQKPDQSPKLLIKYASQSFSGVPSRFSGSGSGTDFTLTIKSLEAEDAATYYCHQSTSLPYTFGQGTKLEIK', 'EVQLVQSGAEVKKPGESLRISCKGSGYSFTGSWISWVRQMPGKGLEWVGRTDPSDSYTSYSPSFQGHVTISSDKSITTAYLQWSSLKASDTAMYYCARHDDTSAYRYLQHWGQGTLVTVSS<cls><cls>QSVLTQPPSVSAAPGQKVTISCSGSSSNIGDNYVSWYQQLPGTAPKLLIYENNKRPSGIPDRFSGSKSGTSATLGITGLQTGDEADYYCGTWDSSLSAWVFGGGTKLTVL', 'EVQLVESGGGLVQPGGSLRLSCAASGFTVSSSFMTWVRQAPGKGLEWVSLLYSGGNTYYADSVKGRFTISRDNSKNTLHLQMNSLRAEDTAVYYCAREPYNDYGNYVAHVFDIWGQGTMVTVSS<cls><cls>EIVMTQSPATLSVSPGERATLSCRASQSVSSHLAWYQQKPGQAPRLLIYGASTRATGIPARFSGSGSGTEFTLIISSLQSEDFALYYCQQYNNWPLTFGGGTKVEIK', 'EVQLVESGGGLIQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGLEWVSVIYSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCASHDDSSGIDYYYGMDVWGQGTTVTVSS<cls><cls>QAVVTQEPSLTVSPGGTVTLTCGSSTGAVTSGHYPYWFQQKPGQAPRTLIYDTSNKHSWTPARFSGSLLGGKAALTLSGAQPEDEAEYYCLLSYSGPWVFGGGTKLTVL', 'EVQLVESGGVVVQPGGSLRLSCAASGFTFDDYTMHWVRQAPGKGLEWVSLISWDGGSTYYADSVKGRFTISRDNSKNSLYLQMNSLRTEDTALYYCAKDIGYDSSGPGHWGQGTLVTVSS<cls><cls>QSVLTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPKLLIYGNSNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSYDSSLSGWVFGGGTKLTVL', 'QVQLQQWGAGLLKPSETLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARGRVVVVPAAPYYYYYYMDVWGKGTTVTVSS<cls><cls>DIQMTQSPSSLSASVGDRVTITCRASQSISSYLNWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTLPFTFGPGTKVDIK', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKGPTTIAAAGPFDYWGQGTLVTVSS<cls><cls>DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKPGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTISSLQAEDVAVYYCQQYYSTPTFGQGTKVEIK', 'QVQLQQWGTGLLKPSETLSLTCAVYGGSFSGFYWSWIRQSPGKGLEWIGEINHNRNTTYNPSLKKRVTISVDTSKNHFYLKVSSVTAADTATYYCAISYGSGPSFDYWGQGTLVFVSS<cls><cls>IQMTQSPSSVSASVGDTVTITCRASHNVDSWLAWYQQKPGKPPKLIIYTASNLQSGVPSRFSGSGSGTDFTLSISGLQPEDSATYFCQQANSFPRTFGQGTKIEI', 'QITLKESGPTLVKPTQTLTLTCTFSGFSLSTSGVGVGWIRQPPGKALEWLALIYWDDDKRYSPSLKSRLTITKDTSKNQVVLTMTNMDPVDTATYYCAHRRITMVRDDAFDIWGQGTMVTVSS<cls><cls>DIQMTQSPSAMSASVGDRVTITCRASQGISNYLAWFQQKPGKVPKRLIYASSSLQSGVPSRFSGSGSGTEFTLTISSLQPEDFATYYCLQHNSYPWTFGQGTKVEIK', 'QVHLVQSGGGVVQPGRSLRLSCAASGYTFTTYGIHWVRQAPGQGLEWVAVIWHDGSREYYLDSVKGRFVVSKDDSKNTVYLQMNSLRVEDTALYFCARDFWGQDYKILDYWGQGALVTVSS<cls><cls>QSVLTQPPSASGTPGQRVTISCSGSRSNIGSNTVNWYQQLPGTTPKLLIYGHNQRPSGVPDRFSGSQSGTSASLAISGLQSEDEADYSCAAWDDRRNGYVFGTGTKVTVL', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYGISWVRQAPGQGLEWMGWISAYNGNTNYAQKLQGRVTMTTDTSTSTAYMELRSLRSDDTAVYYCARVGGVYPYYYYGMDVWGQGTTVTVSS<cls><cls>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPEDFAVYYCQQYGSSPQTFGQGTKVEIK', 'EVQLVESGGGLVQPGGSLRLSCSASGFTFSSYAMHWVRQAPGKGLEYVSAISSNGGSTYYADSVKGRFTISRDNSKNTLYLQMSSLRAEDTAVYYCVKDTTITGTTYPYYYYYMDVWGKGTTVTVSS<cls><cls>DIQMTQSPSSLSASVGDRVTITCRASQSISSYLNWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTPRTFGQGTKLEIK', 'EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAIYYCAKVLGSGYQGDYWGQGTLVTVSS<cls><cls>DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKPGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTISSLQAEDVAVYYCQQYYSTPRFTFGPGTKVDIK', 'QVQLQQWGAGLLKPSETLSLTCAVYGGSFSSYYWRWIRQSPGKGLEWIGEITHSGSTNYNPSLKSRVTISVDRSKNQFSLKLTSVTAADTAVYYCARGRRQQLIRTQWDWFDPWGQGTLVTVSS<cls><cls>DIQMTQSPTSLSASVGDRVTITCRASQSISTYLNWYQQKPGKAPKFLIYGASTLESGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSASPYTFGQGTKLEIK', 'QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGRINPNSGGTNYAQKFQGRVTMTRDTSISTAYMELSRLRSDDTAVYYCARVRWYGGLEGLFDYWGQGTLVTVSS<cls><cls>AIQLTQSPSSLSASVGDRVTITCRASQGISSALAWYQQKPGKAPKLLIYDASSLESGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQFNSYPITFGQGTRLEIK'], 'input_ids': tensor([[ 0, 16,  7,  ...,  1,  1,  1],\n",
      "        [ 0, 16,  7,  ...,  1,  1,  1],\n",
      "        [ 0, 16,  7,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  9,  7,  ...,  1,  1,  1],\n",
      "        [ 0,  9,  7,  ...,  1,  1,  1],\n",
      "        [ 0,  9,  7,  ...,  1,  1,  1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/git/BALM/balm/training/trainer.py:161\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m--> 161\u001b[0m     collated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator(batch)\n\u001b[1;32m    162\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m collated[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    163\u001b[0m     labels \u001b[38;5;241m=\u001b[39m collated\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/git/BALM/balm/data.py:163\u001b[0m, in \u001b[0;36mDataCollator.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    161\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: examples}\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m    164\u001b[0m         examples \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(e, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[1;32m    165\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack(examples)}\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b5fd91410943cc966dc376bf6bfd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, total Loss: 0.3494, LM loss: 0.3329, router z loss: 0.0066, router aux loss: 0.0099  \n",
      "step: 10, total Loss: 0.3194, LM loss: 0.3092, router z loss: 0.0005, router aux loss: 0.0098  \n",
      "step: 15, total Loss: 0.3203, LM loss: 0.3099, router z loss: 0.0004, router aux loss: 0.0100  \n",
      "step: 20, total Loss: 0.3068, LM loss: 0.2969, router z loss: 0.0003, router aux loss: 0.0096  \n",
      "step: 25, total Loss: 0.2884, LM loss: 0.2781, router z loss: 0.0007, router aux loss: 0.0096  \n",
      "step: 30, total Loss: 0.3100, LM loss: 0.2995, router z loss: 0.0008, router aux loss: 0.0097  \n",
      "step: 35, total Loss: 0.2651, LM loss: 0.2547, router z loss: 0.0006, router aux loss: 0.0098  \n",
      "step: 40, total Loss: 0.2867, LM loss: 0.2759, router z loss: 0.0007, router aux loss: 0.0100  \n",
      "step: 45, total Loss: 0.2733, LM loss: 0.2627, router z loss: 0.0007, router aux loss: 0.0099  \n",
      "step: 50, total Loss: 0.2584, LM loss: 0.2482, router z loss: 0.0005, router aux loss: 0.0097  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# outputs = model(\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     input_ids=collated[\"input_ids\"],\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     labels=collated.get(\"labels\", None),\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     key_padding_mask=collated.get(\"attention_mask\", None),\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# pbar.update(train_dataloader.batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "model.train()\n",
    "# pbar = tqdm(total=len(train_dataloader) * train_dataloader.batch_size * n_epochs)\n",
    "pbar = tqdm(total=len(train_dataloader) * n_epochs)\n",
    "n_steps = 0\n",
    "pbar.reset()\n",
    "for epoch in range(n_epochs):\n",
    "    for examples in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        collated = collator(examples[\"input_ids\"])\n",
    "\n",
    "        input_ids = collated[\"input_ids\"].to(device)\n",
    "        labels = collated.get(\"labels\", None)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "        attention_mask = collated.get(\"attention_mask\", None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, key_padding_mask=attention_mask)\n",
    "\n",
    "        # outputs = model(\n",
    "        #     input_ids=collated[\"input_ids\"],\n",
    "        #     labels=collated.get(\"labels\", None),\n",
    "        #     key_padding_mask=collated.get(\"attention_mask\", None),\n",
    "        # )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # pbar.update(train_dataloader.batch_size)\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "        n_steps += 1\n",
    "        if n_steps % 5 == 0:\n",
    "            print(\n",
    "                f\"step: {n_steps}, total Loss: {loss.item():.4f}, LM loss: {outputs.lm_loss.item():.4f}, router z loss: {outputs.router_z_loss.item():.4f}, router aux loss: {outputs.router_aux_loss.item():.4f}  \"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
