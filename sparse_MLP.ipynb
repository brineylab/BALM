{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from balm.router import ExpertChoiceRouter, TopKRouter\n",
    "from balm.modules import Expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a Sparse MLP module, for use in Mixture-of-Experts models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    ffn_dim : int\n",
    "        Feedforward dimension.\n",
    "\n",
    "    num_experts : int\n",
    "        Number of experts.\n",
    "\n",
    "    expert_capacity : int\n",
    "        Capacity of each expert.\n",
    "\n",
    "    top_k : int, optional\n",
    "        Top k for the router. The default is 1.\n",
    "\n",
    "    activation : str, optional\n",
    "        Activation function to use. The default is \"swiglu\".\n",
    "\n",
    "    expert_ffn_dropout : float, optional\n",
    "        Dropout rate for the expert feedforward layer. The default is 0.0.\n",
    "\n",
    "    router_dtype : str, optional\n",
    "        Dtype for the router. The default is \"float32\".\n",
    "\n",
    "    router_bias : bool, optional\n",
    "        Whether to use bias for the router. The default is False.\n",
    "\n",
    "    router_jitter : float, optional\n",
    "        Jitter for the router. The default is 0.0.\n",
    "\n",
    "    router_ignore_padding_tokens : bool, optional\n",
    "        Whether to ignore padding tokens for the router. The default is True.\n",
    "\n",
    "    router_class : nn.Module, optional\n",
    "        Router class to use. The default is ``TopKRouter``.\n",
    "\n",
    "    expert_class : nn.Module, optional\n",
    "        Expert class to use. The default is ``Expert``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        ffn_dim: int,\n",
    "        num_experts: int,\n",
    "        expert_capacity: int,\n",
    "        num_shared_experts: int = 0,\n",
    "        top_k: int = 1,\n",
    "        activation: str = \"swiglu\",\n",
    "        expert_ffn_dropout: float = 0.0,\n",
    "        router_dtype: str = \"float32\",\n",
    "        router_bias: bool = False,\n",
    "        router_jitter: float = 0.0,\n",
    "        router_ignore_padding_tokens: bool = True,\n",
    "        router_class: nn.Module = TopKRouter,\n",
    "        expert_class: nn.Module = Expert,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.router = router_class(\n",
    "            embed_dim=embed_dim,\n",
    "            num_experts=num_experts,\n",
    "            expert_capacity=expert_capacity,\n",
    "            top_k=top_k,\n",
    "            num_shared_experts=num_shared_experts,\n",
    "            dtype=router_dtype,\n",
    "            bias=router_bias,\n",
    "            jitter=router_jitter,\n",
    "            ignore_padding_tokens=router_ignore_padding_tokens,\n",
    "        )\n",
    "        self.experts = nn.ModuleDict()\n",
    "        for idx in range(num_experts):\n",
    "            self.experts[f\"expert_{idx}\"] = expert_class(\n",
    "                embed_dim=embed_dim,\n",
    "                ffn_dim=ffn_dim,\n",
    "                dropout=expert_ffn_dropout,\n",
    "                activation=activation,\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple]:\n",
    "        \"\"\"\n",
    "        Route tokens to experts and process them.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        output : Tuple[torch.Tensor, Tuple]\n",
    "            A tuple containing the following:\n",
    "             - x : torch.Tensor\n",
    "                Output tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "             - router_outputs : Tuple[torch.Tensor, torch.Tensor]\n",
    "                A tuple containing the following:\n",
    "                 - router_logits : torch.Tensor\n",
    "                    Router logits of shape (batch_size, sequence_length, num_experts).\n",
    "                 - expert_mask : torch.Tensor\n",
    "                    Expert mask of shape (batch_size, sequence_length, num_experts).\n",
    "        \"\"\"\n",
    "        # router\n",
    "        expert_mask, router_probs, router_logits = self.router(x)\n",
    "        expert_outputs = []\n",
    "\n",
    "        # experts\n",
    "        for idx, expert in self.experts.items():\n",
    "            int_idx = int(idx.split(\"_\")[-1])\n",
    "            token_indices = expert_mask[..., int_idx].bool()\n",
    "            expert_output = expert(x[token_indices]).to(x.dtype)\n",
    "            expanded_output = torch.zeros_like(x)\n",
    "            expanded_output[token_indices] = expert_output\n",
    "            expert_outputs.append(expanded_output)\n",
    "\n",
    "        # combine outputs from the selected tokens for each expert\n",
    "        x = torch.stack(expert_outputs, dim=-1) * expert_mask.unsqueeze(-2)\n",
    "        x = x.sum(dim=-1)\n",
    "\n",
    "        return x, (router_logits, expert_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.rand(10, 96, 320)\n",
    "sparse_mlp = SparseMLP(embed_dim=320, ffn_dim=320*4, num_experts=16, expert_capacity=320/16*1.5)\n",
    "output = sparse_mlp(input_tensor)\n",
    "print(output[0].shape)\n",
    "print(output[1][0].shape)\n",
    "print(output[1][1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
